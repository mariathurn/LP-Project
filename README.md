Multilingual Text Detoxification

This repository contains code, models, and evaluation scripts for a project on multilingual text detoxification, focusing on English and German. The aim is to reduce toxicity in text while preserving semantic content and fluency.

ğŸ“Œ Overview

We compare two detoxification approaches:
	â€¢	Delete-Only Baseline: A rule-based method that removes predefined toxic spans from the input.
	â€¢	Refined Model: A fine-tuned t5-small model trained on English toxic/non-toxic pairs from the s-nlp/paradetox dataset.

German detoxification is evaluated using a translation-based pipeline: German â†’ English â†’ detoxify â†’ German.

ğŸ“ Project Structure

â”œâ”€â”€ main.py                          # Delete-only baseline
â”œâ”€â”€ Model-refined.ipynb              # Translation-based detoxification with refined T5 model
â”œâ”€â”€ xcoment_fluency.ipynb            # Fluency evaluation using XCOMET-lite
â”œâ”€â”€ input_data/                      # Datasets in JSON format
â”œâ”€â”€ output_data/                     # Detoxified outputs for both EN and DE
â”œâ”€â”€ evaluation/                      # Evaluation scripts and logs
â””â”€â”€ requirements.txt                 # Required Python packages

ğŸ§ª Evaluation Metrics

The following metrics are used:
	â€¢	Toxicity Score: XLM-Roberta classifier for binary toxicity classification.
	â€¢	Content Preservation: Cosine similarity between LaBSE sentence embeddings.
	â€¢	Fluency: COMET-style XCOMETLite model estimates output fluency without references.

ğŸ“Š Results Summary

Method	EN Non-Toxic (%)	DE Non-Toxic (%)	EN LaBSE	DE LaBSE	EN Fluency	DE Fluency
Delete-Only Baseline	71.25	39.00	0.9478	0.9810	0.6290	0.9424
Refined Model	68.00	56.25	0.9089	0.8848	0.8715	0.8999

ğŸš€ How to Run
	1.	Install dependencies

pip install -r requirements.txt


	2.	Run Delete Baseline

python main.py


	3.	Run Refined Model Detoxification
Open and run Model-refined.ipynb to apply the translation-based pipeline.
	4.	Evaluate Fluency
Run xcoment_fluency.ipynb to compute XCOMETLite scores.

ğŸ“– Citation

If you use this codebase or dataset in your research, please cite relevant references such as:
	â€¢	Dementieva et al. (2023). Exploring methods for cross-lingual text style transfer.
	â€¢	Sushko (2024). Synthetic training for multilingual detoxification.
	â€¢	Luo et al. (2024). Translation-based detox pipelines with post-processing.
