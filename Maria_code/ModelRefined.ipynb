{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a12cdaf1",
   "metadata": {},
   "source": [
    "## Load tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86538e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "import json\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# ---------- Preprocesing Functions ----------\n",
    "def preprocess_paradetox_multilingual(example):\n",
    "    return {\n",
    "        \"input_text\": \"detoxify: \" + example[\"toxic_sentence\"],\n",
    "        \"target_text\": example[\"neutral_sentence\"]\n",
    "    }\n",
    "\n",
    "def preprocess_paradetox(example):\n",
    "    return {\n",
    "        \"input_text\": \"detoxify: \" + example[\"en_toxic_comment\"],\n",
    "        \"target_text\": example[\"en_neutral_comment\"]\n",
    "    }\n",
    "\n",
    "def clean_columns(dataset):\n",
    "    return dataset.remove_columns(\n",
    "        [col for col in dataset.column_names if col not in [\"input_text\", \"target_text\"]]\n",
    "    )\n",
    "\n",
    "\n",
    "def load_json_results(filepath):\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def save_variables_to_json(filename, **variables):\n",
    "    \"\"\"\n",
    "    Saves given variables to a JSON file with their variable names as keys.\n",
    "\n",
    "    Parameters:\n",
    "    - filename (str): The name of the JSON file to write to.\n",
    "    - **variables: Arbitrary keyword arguments representing variable names and their values.\n",
    "    \"\"\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(variables, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3742937a",
   "metadata": {},
   "source": [
    "## Load and Process Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aaeeca73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Load and Process Datasets ----------\n",
    "test_data_en = load_dataset(\"textdetox/multilingual_paradetox\", split=\"en\")\n",
    "test_data_de = load_dataset(\"textdetox/multilingual_paradetox\", split=\"de\")\n",
    "train_data = load_dataset(\"s-nlp/paradetox\", split=\"train\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbf508e",
   "metadata": {},
   "source": [
    "## format datasets remove emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94f6faa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "formatted_train = clean_columns(train_data.map(preprocess_paradetox))\n",
    "formatted_en = clean_columns(test_data_en.map(preprocess_paradetox_multilingual))\n",
    "formatted_de = clean_columns(test_data_de.map(preprocess_paradetox_multilingual))\n",
    "\n",
    "\n",
    "def remove_emojis(text):\n",
    "    # This regex pattern matches a wide range of emojis\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        \"\\U0001F600-\\U0001F64F\"  # Emoticons\n",
    "        \"\\U0001F300-\\U0001F5FF\"  # Symbols & Pictographs\n",
    "        \"\\U0001F680-\\U0001F6FF\"  # Transport & Map Symbols\n",
    "        \"\\U0001F1E0-\\U0001F1FF\"  # Flags\n",
    "        \"\\U00002700-\\U000027BF\"  # Dingbats\n",
    "        \"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "        \"\\U00002600-\\U000026FF\"  # Miscellaneous Symbols\n",
    "        \"\\U0001F700-\\U0001F77F\"  # Alchemical Symbols\n",
    "        \"\\U000024C2-\\U0001F251\"\n",
    "        \"]+\",\n",
    "        flags=re.UNICODE\n",
    "    )\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "def clean_emoji_batch(batch):\n",
    "    batch[\"input_text\"] = remove_emojis(batch[\"input_text\"])\n",
    "    batch[\"target_text\"] = remove_emojis(batch[\"target_text\"])\n",
    "    return batch\n",
    "\n",
    "formatted_train = formatted_train.map(clean_emoji_batch)\n",
    "formatted_en = formatted_en.map(clean_emoji_batch)\n",
    "formatted_de = formatted_de.map(clean_emoji_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f23c81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset en size: 400\n",
      "Test dataset de size: 400\n",
      "Train dataset size: 19744\n",
      "Test en dataset columns: ['input_text', 'target_text']\n",
      "Test de dataset columns: ['input_text', 'target_text']\n",
      "Train dataset columns: ['input_text', 'target_text']\n"
     ]
    }
   ],
   "source": [
    "print(\"Test dataset en size:\", len(formatted_en))\n",
    "print(\"Test dataset de size:\", len(formatted_de))\n",
    "print(\"Train dataset size:\", len(formatted_train))\n",
    "\n",
    "print(\"Test en dataset columns:\", formatted_en.column_names)\n",
    "print(\"Test de dataset columns:\", formatted_de.column_names)\n",
    "print(\"Train dataset columns:\", formatted_train.column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9580fc2",
   "metadata": {},
   "source": [
    "## Save datasets to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ec438fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_variables_to_json(\n",
    "    \"formatted_de.json\",\n",
    "    input_texts=formatted_de[\"input_text\"],\n",
    "    reference_texts=formatted_de[\"target_text\"],\n",
    ")\n",
    "\n",
    "save_variables_to_json(\n",
    "    \"formatted_en.json\",\n",
    "    input_texts=formatted_en[\"input_text\"],\n",
    "    reference_texts=formatted_en[\"target_text\"],\n",
    ")\n",
    "\n",
    "save_variables_to_json(\n",
    "    \"formatted_train.json\",\n",
    "    input_texts=formatted_train[\"input_text\"],\n",
    "    reference_texts=formatted_train[\"target_text\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27701d34",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "463043fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "def load_formatted_json(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Convert keys to the structure used by your pipeline\n",
    "    input_texts = data[\"input_texts\"]\n",
    "    target_texts = data[\"reference_texts\"]\n",
    "\n",
    "    return Dataset.from_dict({\n",
    "        \"input_text\": input_texts,\n",
    "        \"target_text\": target_texts\n",
    "    })\n",
    "\n",
    "formatted_train = load_formatted_json(\"formatted_train.json\")\n",
    "formatted_de = load_formatted_json(\"formatted_de.json\")\n",
    "formatted_en = load_formatted_json(\"formatted_en.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683d7c4d",
   "metadata": {},
   "source": [
    "## Evaluate Toxicity before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4a623f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_toxicity_from_json(file_path: str, input_key: str = \"input_texts\", prefix_to_strip: str = \"detoxify: \"):\n",
    "    \"\"\"\n",
    "    Loads a JSON file with a dictionary of lists, classifies toxicity, and prints stats.\n",
    "    \"\"\"\n",
    "    # Load dataset\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        dataset = json.load(f)\n",
    "\n",
    "    # Directly access the list under the input_key\n",
    "    input_texts = [text.replace(prefix_to_strip, \"\") for text in dataset[input_key]]\n",
    "\n",
    "    print(f\"✅ Loaded {len(input_texts)} texts from {file_path}\")\n",
    "\n",
    "    # Run classification in batches\n",
    "    batch_size = 16\n",
    "    toxic_count = 0\n",
    "    non_toxic_count = 0\n",
    "\n",
    "    for i in range(0, len(input_texts), batch_size):\n",
    "        batch = input_texts[i:i + batch_size]\n",
    "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        predictions = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "        for label in predictions:\n",
    "            if label.item() == 0:\n",
    "                non_toxic_count += 1\n",
    "            else:\n",
    "                toxic_count += 1\n",
    "\n",
    "    total = toxic_count + non_toxic_count\n",
    "    print(\"\\n--- Toxicity Evaluation ---\")\n",
    "    print(f\"Total inputs evaluated: {total}\")\n",
    "    print(f\"Non-toxic: {non_toxic_count} ({(non_toxic_count / total) * 100:.2f}%)\")\n",
    "    print(f\"Toxic: {toxic_count} ({(toxic_count / total) * 100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b3714e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 400 texts from formatted_en.json\n",
      "\n",
      "--- Toxicity Evaluation ---\n",
      "Total inputs evaluated: 400\n",
      "Non-toxic: 31 (7.75%)\n",
      "Toxic: 369 (92.25%)\n",
      "✅ Loaded 400 texts from formatted_de.json\n",
      "\n",
      "--- Toxicity Evaluation ---\n",
      "Total inputs evaluated: 400\n",
      "Non-toxic: 109 (27.25%)\n",
      "Toxic: 291 (72.75%)\n"
     ]
    }
   ],
   "source": [
    "evaluate_toxicity_from_json(\"formatted_en.json\")\n",
    "evaluate_toxicity_from_json(\"formatted_de.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0a7c01fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c890e9c8f8be4252a982c45515bcc8f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/19744 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonaz\\git\\Maria_stuff\\Maria_code\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:4109: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a427b265850b408398da056dd1fb3068",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/19744 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b7602657d5546aaacd0d6a3eec7ef9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61900b92228b49b3bc801fa0dc638bf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize(examples):\n",
    "    # Tokenize inputs\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"input_text\"],\n",
    "        max_length=128,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    # Tokenize targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            examples[\"target_text\"],\n",
    "            max_length=128,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "    # Mask padding tokens in labels\n",
    "    labels[\"input_ids\"] = [\n",
    "        [(token if token != tokenizer.pad_token_id else -100) for token in label_seq]\n",
    "        for label_seq in labels[\"input_ids\"]\n",
    "    ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Tokenize dataset\n",
    "tokenized_dataset = formatted_train.map(tokenize, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.filter(lambda x: any(label != -100 for label in x[\"labels\"]))\n",
    "\n",
    "tokenized_eval = formatted_en.map(tokenize, batched=True)\n",
    "tokenized_eval = tokenized_eval.filter(lambda x: any(label != -100 for label in x[\"labels\"]))\n",
    "\n",
    "# Assign to eval_dataset\n",
    "eval_dataset = tokenized_eval\n",
    "train_dataset = tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "328b75c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 19744\n",
      "Eval dataset size: 400\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "{'input_text': \"detoxify: My page should be protected first so that worthless scum like you can't keep vandalizing it.\", 'target_text': \"My page should be protected first so that unpleasant people like you can't keep vandalizing it.\", 'input_ids': [16379, 4921, 10, 499, 543, 225, 36, 5046, 166, 78, 24, 1494, 924, 3, 7, 6361, 114, 25, 54, 31, 17, 453, 4049, 26, 138, 2610, 34, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [499, 543, 225, 36, 5046, 166, 78, 24, 24276, 151, 114, 25, 54, 31, 17, 453, 4049, 26, 138, 2610, 34, 5, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "{'input_text': 'detoxify: dude should have been taken to api , he would be right at home with all the other knuckleheads there', 'target_text': 'It would have been good if he went to api. He would fit in.', 'input_ids': [16379, 4921, 10, 146, 221, 225, 43, 118, 1026, 12, 3, 13306, 3, 6, 3, 88, 133, 36, 269, 44, 234, 28, 66, 8, 119, 3, 157, 29, 4636, 109, 3313, 7, 132, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [94, 133, 43, 118, 207, 3, 99, 3, 88, 877, 12, 3, 13306, 5, 216, 133, 1400, 16, 5, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}\n"
     ]
    }
   ],
   "source": [
    "print(\"Train dataset size:\", len(train_dataset))\n",
    "print(\"Eval dataset size:\", len(eval_dataset))\n",
    "\n",
    "print(type(eval_dataset))\n",
    "print(eval_dataset[1])\n",
    "\n",
    "print(type(train_dataset))\n",
    "print(train_dataset[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8218fa4",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7eaef8",
   "metadata": {},
   "source": [
    "## old transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42bd61fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f8cd26769fe4c528407b3e8008ff5b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3702 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'penalty_score': 0.8384648561477661, 'epoch': 0}\n",
      "{'penalty_score': 0.7225239276885986, 'epoch': 0}\n",
      "{'penalty_score': 0.7299094200134277, 'epoch': 0.0}\n",
      "{'penalty_score': 0.9040523767471313, 'epoch': 0.0}\n",
      "{'penalty_score': 0.6879165172576904, 'epoch': 0.0}\n",
      "{'penalty_score': 0.6071668863296509, 'epoch': 0.0}\n",
      "{'penalty_score': 0.8161963224411011, 'epoch': 0.0}\n",
      "{'penalty_score': 0.677609920501709, 'epoch': 0.0}\n",
      "{'penalty_score': 0.6595351696014404, 'epoch': 0.0}\n",
      "{'penalty_score': 0.8427661657333374, 'epoch': 0.0}\n",
      "{'penalty_score': 0.7256010174751282, 'epoch': 0.0}\n",
      "{'penalty_score': 0.4845339059829712, 'epoch': 0.0}\n",
      "{'penalty_score': 0.8149999380111694, 'epoch': 0.0}\n",
      "{'penalty_score': 0.7716923356056213, 'epoch': 0.0}\n",
      "{'penalty_score': 0.6034647822380066, 'epoch': 0.01}\n",
      "{'penalty_score': 0.653598427772522, 'epoch': 0.01}\n",
      "{'penalty_score': 0.49958038330078125, 'epoch': 0.01}\n",
      "{'penalty_score': 0.8837884664535522, 'epoch': 0.01}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 102\u001b[39m\n\u001b[32m     90\u001b[39m \u001b[38;5;66;03m# ✅ Instantiate and train\u001b[39;00m\n\u001b[32m     91\u001b[39m trainer = ToxicityPenaltyTrainer(\n\u001b[32m     92\u001b[39m     model=model,\n\u001b[32m     93\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m     99\u001b[39m     lambda_penalty=\u001b[32m0.9\u001b[39m\n\u001b[32m    100\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jonaz\\git\\Maria_stuff\\Maria_code\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2052\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2050\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2051\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2052\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2053\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2054\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2055\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2056\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2057\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jonaz\\git\\Maria_stuff\\Maria_code\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2388\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2385\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_step_begin(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n\u001b[32m   2387\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.accumulate(model):\n\u001b[32m-> \u001b[39m\u001b[32m2388\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2390\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2391\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2392\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2393\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2394\u001b[39m ):\n\u001b[32m   2395\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2396\u001b[39m     tr_loss += tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jonaz\\git\\Maria_stuff\\Maria_code\\.venv\\Lib\\site-packages\\transformers\\trainer.py:3485\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs)\u001b[39m\n\u001b[32m   3482\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb.reduce_mean().detach().to(\u001b[38;5;28mself\u001b[39m.args.device)\n\u001b[32m   3484\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_context_manager():\n\u001b[32m-> \u001b[39m\u001b[32m3485\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3487\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[32m   3488\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   3489\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3490\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.global_step % \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps == \u001b[32m0\u001b[39m\n\u001b[32m   3491\u001b[39m ):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 60\u001b[39m, in \u001b[36mToxicityPenaltyTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, **kwargs)\u001b[39m\n\u001b[32m     57\u001b[39m current_lambda = \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m.lambda_penalty, \u001b[38;5;28mself\u001b[39m.lambda_penalty * \u001b[38;5;28mself\u001b[39m.current_step / warmup_steps)\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m     generated_ids = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m        \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecoder_start_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpad_token_id\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     70\u001b[39m     decoded_texts = \u001b[38;5;28mself\u001b[39m.tokenizer.batch_decode(generated_ids, skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     71\u001b[39m     tox_inputs = \u001b[38;5;28mself\u001b[39m.tox_tokenizer(decoded_texts, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m, truncation=\u001b[38;5;28;01mTrue\u001b[39;00m, padding=\u001b[38;5;28;01mTrue\u001b[39;00m).to(\u001b[38;5;28mself\u001b[39m.model.device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jonaz\\git\\Maria_stuff\\Maria_code\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jonaz\\git\\Maria_stuff\\Maria_code\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:2078\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[39m\n\u001b[32m   2070\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2071\u001b[39m         input_ids=input_ids,\n\u001b[32m   2072\u001b[39m         expand_size=generation_config.num_beams,\n\u001b[32m   2073\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2074\u001b[39m         **model_kwargs,\n\u001b[32m   2075\u001b[39m     )\n\u001b[32m   2077\u001b[39m     \u001b[38;5;66;03m# 13. run beam sample\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2078\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_beam_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2079\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2080\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeam_scorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2081\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2082\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2083\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2084\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2085\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2086\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2088\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode == GenerationMode.GROUP_BEAM_SEARCH:\n\u001b[32m   2089\u001b[39m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[32m   2090\u001b[39m     beam_scorer = BeamSearchScorer(\n\u001b[32m   2091\u001b[39m         batch_size=batch_size,\n\u001b[32m   2092\u001b[39m         num_beams=generation_config.num_beams,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2098\u001b[39m         max_length=generation_config.max_length,\n\u001b[32m   2099\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jonaz\\git\\Maria_stuff\\Maria_code\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:3314\u001b[39m, in \u001b[36mGenerationMixin._beam_search\u001b[39m\u001b[34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[39m\n\u001b[32m   3311\u001b[39m next_tokens = next_tokens % vocab_size\n\u001b[32m   3313\u001b[39m \u001b[38;5;66;03m# stateless\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3314\u001b[39m beam_outputs = \u001b[43mbeam_scorer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3315\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3316\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnext_token_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3317\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnext_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3318\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnext_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3319\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3320\u001b[39m \u001b[43m    \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3321\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeam_indices\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeam_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3322\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_prompt_len\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_prompt_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3323\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3325\u001b[39m beam_scores = beam_outputs[\u001b[33m\"\u001b[39m\u001b[33mnext_beam_scores\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   3326\u001b[39m beam_next_tokens = beam_outputs[\u001b[33m\"\u001b[39m\u001b[33mnext_beam_tokens\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jonaz\\git\\Maria_stuff\\Maria_code\\.venv\\Lib\\site-packages\\transformers\\generation\\beam_search.py:264\u001b[39m, in \u001b[36mBeamSearchScorer.process\u001b[39m\u001b[34m(self, input_ids, next_scores, next_tokens, next_indices, pad_token_id, eos_token_id, beam_indices, group_index, decoder_prompt_len)\u001b[39m\n\u001b[32m    262\u001b[39m     next_beam_tokens[batch_idx, :] = pad_token_id\n\u001b[32m    263\u001b[39m     next_beam_indices[batch_idx, :] = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m264\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    266\u001b[39m \u001b[38;5;66;03m# next tokens for this sentence\u001b[39;00m\n\u001b[32m    267\u001b[39m beam_idx = \u001b[32m0\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    DataCollatorForSeq2Seq,\n",
    "    EarlyStoppingCallback,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Seq2SeqTrainer,\n",
    ")\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./mt5-detox-en-finetuned\",\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=3e-5,\n",
    "    num_train_epochs=3,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    logging_dir=\"./logs_en_refined\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# ✅ Custom Trainer with full-batch penalty, lambda warmup, and logging\n",
    "class ToxicityPenaltyTrainer(Seq2SeqTrainer):\n",
    "    def __init__(self, *args, lambda_penalty=1.5, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.lambda_penalty = lambda_penalty\n",
    "        self.tox_tokenizer = AutoTokenizer.from_pretrained(\"unitary/toxic-bert\")\n",
    "        self.tox_model = AutoModelForSequenceClassification.from_pretrained(\"unitary/toxic-bert\").to(self.model.device)\n",
    "        self.tox_model.eval()\n",
    "        self.current_step = 0  # For lambda warmup\n",
    "        self.penalty_log = []\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        outputs = model(**inputs)\n",
    "        generation_loss = outputs.loss\n",
    "\n",
    "        # Full batch generation for penalty\n",
    "        sample_input_ids = inputs[\"input_ids\"]\n",
    "        sample_attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "        self.current_step += 1\n",
    "        warmup_steps = 500\n",
    "        current_lambda = min(self.lambda_penalty, self.lambda_penalty * self.current_step / warmup_steps)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            generated_ids = model.generate(\n",
    "                input_ids=sample_input_ids,\n",
    "                attention_mask=sample_attention_mask,\n",
    "                max_length=30,\n",
    "                num_beams=4, \n",
    "                do_sample=False,\n",
    "                early_stopping=True,\n",
    "                decoder_start_token_id=self.tokenizer.pad_token_id\n",
    "            )\n",
    "\n",
    "            decoded_texts = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "            tox_inputs = self.tox_tokenizer(decoded_texts, return_tensors=\"pt\", truncation=True, padding=True).to(self.model.device)\n",
    "            tox_logits = self.tox_model(**tox_inputs).logits\n",
    "            tox_probs = torch.sigmoid(tox_logits[:, 0])\n",
    "            penalty = tox_probs.mean()\n",
    "\n",
    "        # Log penalty score\n",
    "        self.log({\"penalty_score\": penalty.item()})\n",
    "        self.penalty_log.append({\n",
    "            \"penalty_score\": penalty.item(),\n",
    "            \"generation_loss\": generation_loss.item(),\n",
    "            \"step\": self.state.global_step,\n",
    "            \"epoch\": self.state.epoch\n",
    "        })\n",
    "\n",
    "        # Scaled penalty\n",
    "        penalty_weight = torch.tanh(current_lambda * penalty)\n",
    "        total_loss = generation_loss + penalty_weight\n",
    "        return (total_loss, outputs) if return_outputs else total_loss\n",
    "\n",
    "# ✅ Instantiate and train\n",
    "trainer = ToxicityPenaltyTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    "    lambda_penalty=0.9\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84a0d4a",
   "metadata": {},
   "source": [
    "## Finetuned Finetuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966310c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import (\n",
    "#     DataCollatorForSeq2Seq,\n",
    "#     EarlyStoppingCallback,\n",
    "#     Seq2SeqTrainingArguments,\n",
    "#     AutoTokenizer,\n",
    "#     AutoModelForSequenceClassification,\n",
    "#     Seq2SeqTrainer,\n",
    "# )\n",
    "# from datasets import Dataset\n",
    "# import torch\n",
    "# import warnings\n",
    "\n",
    "# warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# # Ensure decoder start token is set\n",
    "# if model.config.decoder_start_token_id is None:\n",
    "#     model.config.decoder_start_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# # Data collator\n",
    "# data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "# # Training arguments\n",
    "# training_args = Seq2SeqTrainingArguments(\n",
    "#     output_dir=\"./mt5-detox-en-finetuned_2\",\n",
    "#     overwrite_output_dir=True,\n",
    "#     per_device_train_batch_size=8,\n",
    "#     gradient_accumulation_steps=2,\n",
    "#     learning_rate=3e-5,\n",
    "#     num_train_epochs=3,\n",
    "#     fp16=torch.cuda.is_available(),\n",
    "#     save_strategy=\"epoch\",\n",
    "#     eval_strategy=\"epoch\",\n",
    "#     logging_strategy=\"steps\",\n",
    "#     logging_steps=100,\n",
    "#     save_total_limit=2,\n",
    "#     load_best_model_at_end=True,\n",
    "#     metric_for_best_model=\"eval_loss\",\n",
    "#     greater_is_better=False,\n",
    "#     logging_dir=\"./logs_en_refined\",\n",
    "#     report_to=\"none\",\n",
    "#     label_smoothing_factor=0.1,\n",
    "#     generation_num_beams=4  # Ensure compatibility with early_stopping\n",
    "# )\n",
    "\n",
    "# # Custom Trainer\n",
    "# class ToxicityPenaltyTrainer(Seq2SeqTrainer):\n",
    "#     def __init__(self, *args, lambda_penalty=1.5, penalty_interval=1, **kwargs):\n",
    "#         super().__init__(*args, **kwargs)\n",
    "#         self.lambda_penalty = lambda_penalty\n",
    "#         self.penalty_interval = penalty_interval\n",
    "#         self.tox_tokenizer = AutoTokenizer.from_pretrained(\"unitary/toxic-bert\")\n",
    "#         self.tox_model = AutoModelForSequenceClassification.from_pretrained(\"unitary/toxic-bert\").to(self.model.device)\n",
    "#         self.tox_model.eval()\n",
    "#         self.current_step = 0\n",
    "#         self.penalty_log = []\n",
    "\n",
    "#     def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "#         outputs = model(**inputs)\n",
    "#         generation_loss = outputs.loss\n",
    "\n",
    "#         self.current_step += 1\n",
    "#         warmup_steps = 500\n",
    "#         current_lambda = self.lambda_penalty * min(1.0, (self.current_step / warmup_steps)) ** 0.7\n",
    "\n",
    "#         if self.current_step % self.penalty_interval == 0:\n",
    "#             with torch.no_grad():\n",
    "#                 generated_ids = model.generate(\n",
    "#                     input_ids=inputs[\"input_ids\"],\n",
    "#                     attention_mask=inputs[\"attention_mask\"],\n",
    "#                     max_length=30,\n",
    "#                     num_beams=4,\n",
    "#                     do_sample=False,\n",
    "#                     early_stopping=True,\n",
    "#                     decoder_start_token_id=self.tokenizer.pad_token_id\n",
    "#                 )\n",
    "\n",
    "#                 decoded_texts = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "#                 tox_inputs = self.tox_tokenizer(\n",
    "#                     decoded_texts,\n",
    "#                     return_tensors=\"pt\",\n",
    "#                     truncation=True,\n",
    "#                     padding=True\n",
    "#                 ).to(self.model.device)\n",
    "\n",
    "#                 tox_logits = self.tox_model(**tox_inputs).logits\n",
    "#                 tox_probs = torch.sigmoid(tox_logits[:, 0])\n",
    "#                 penalty = tox_probs.mean()\n",
    "#         else:\n",
    "#             penalty = torch.tensor(0.0, device=self.model.device)\n",
    "\n",
    "#         self.log({\"penalty_score\": penalty.item()})\n",
    "#         self.penalty_log.append({\n",
    "#             \"penalty_score\": penalty.item(),\n",
    "#             \"generation_loss\": generation_loss.item(),\n",
    "#             \"step\": self.state.global_step,\n",
    "#             \"epoch\": self.state.epoch\n",
    "#         })\n",
    "\n",
    "#         total_loss = generation_loss + current_lambda * penalty\n",
    "#         return (total_loss, outputs) if return_outputs else total_loss\n",
    "\n",
    "# \"\"\" # Reduce dataset size for quick testing\n",
    "# if isinstance(train_dataset, dict):\n",
    "#     train_dataset = Dataset.from_dict(train_dataset)\n",
    "# train_dataset = train_dataset.select(range(min(500, len(train_dataset)))) \"\"\"\n",
    "\n",
    "# # Instantiate and train\n",
    "# trainer = ToxicityPenaltyTrainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=eval_dataset,\n",
    "#     tokenizer=tokenizer,\n",
    "#     data_collator=data_collator,\n",
    "#     callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    "#     lambda_penalty=1.5\n",
    "# )\n",
    "\n",
    "# trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b8d58e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Save logs to JSON\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mtrainer_log.json\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     json.dump(\u001b[43mtrainer\u001b[49m.state.log_history, f, indent=\u001b[32m2\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# ✅ Now read logs back from file\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mtrainer_log.json\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[31mNameError\u001b[39m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "# Save logs to JSON\n",
    "with open(\"trainer_log.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(trainer.state.log_history, f, indent=2)\n",
    "\n",
    "# ✅ Now read logs back from file\n",
    "with open(\"trainer_log.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    logs = json.load(f)\n",
    "\n",
    "# Extract and plot penalty scores\n",
    "penalties = [log[\"penalty_score\"] for log in logs if \"penalty_score\" in log]\n",
    "plt.plot(penalties)\n",
    "plt.xlabel(\"Training step\")\n",
    "plt.ylabel(\"Penalty Score\")\n",
    "plt.title(\"Toxicity Penalty Over Time\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648392f5",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b92646b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "# Path to your saved model\n",
    "checkpoint_path = \"mt5-detox-en-finetuned_2/checkpoint-3702\"\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(checkpoint_path)\n",
    "model = T5ForConditionalGeneration.from_pretrained(checkpoint_path)\n",
    "\n",
    "# Send to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a52b06",
   "metadata": {},
   "source": [
    "## English Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "42e01148",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:26<00:00,  1.89it/s]\n",
      "100%|██████████| 50/50 [00:25<00:00,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Translated dataset saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "# Load German-to-English model\n",
    "model_name = \"Helsinki-NLP/opus-mt-de-en\"\n",
    "tokenizer_mt = MarianTokenizer.from_pretrained(model_name)\n",
    "model_mt = MarianMTModel.from_pretrained(model_name).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Use formatted_de directly (already loaded as a Dataset)\n",
    "toxic_de = [text.replace(\"detoxify: \", \"\") for text in formatted_de[\"input_text\"]]\n",
    "neutral_de = list(formatted_de[\"target_text\"])\n",
    "\n",
    "# Helper to batch translate a list of strings\n",
    "def batch_translate(texts, batch_size=8):\n",
    "    translations = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size)):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        inputs = tokenizer_mt(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=128).to(model_mt.device)\n",
    "        outputs = model_mt.generate(**inputs, max_length=128)\n",
    "        decoded = tokenizer_mt.batch_decode(outputs, skip_special_tokens=True)\n",
    "        translations.extend(decoded)\n",
    "    return translations\n",
    "\n",
    "# Translate both\n",
    "toxic_en = batch_translate(toxic_de)\n",
    "neutral_en = batch_translate(neutral_de)\n",
    "\n",
    "# Reconstruct translated dataset\n",
    "translated_data = [\n",
    "    {\"input_text\": \"detoxify: \" + toxic, \"target_text\": neutral}\n",
    "    for toxic, neutral in zip(toxic_en, neutral_en)\n",
    "]\n",
    "\n",
    "# Save to file\n",
    "with open(\"translated_de_to_en_pairs.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(translated_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"✅ Translated dataset saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7bd963",
   "metadata": {},
   "source": [
    "## Load German translated to english Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba4379dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw texts loaded: [{'input_text': 'detoxify: #Land of the idiots #mental illness - \"fight gg #Trump\" (gg #Putin anyway) German # Regime in megalomania ....', 'target_text': '\"Battle gg #Trump\" (gg #Putin anyway) German # Regime in megalomania ....'}, {'input_text': \"detoxify: #Lindner's got his head in the ass of #Merkel disgusting #FDP tonight\", 'target_text': '#Lindner flatters himself again tonight #Merkel #FDP'}, {'input_text': \"detoxify: Whether it's Merkel or Schulz? We Germans don't want to and can't hear this moron of plague (Merkel) and cholera (Schulz) anymore!\", 'target_text': \"Whether it's Merkel or Schulz? We Germans don't want and can't hear this talk from Merkel and Schulz anymore!\"}, {'input_text': 'detoxify: Such a shabby rag, leaves children alone in war and flag flight', 'target_text': 'Such a person, leaves children in war alone and flag flight'}, {'input_text': \"detoxify: I don't think the majority of the population is stupid or just come to your appearances?\", 'target_text': \"I don't think the majority of the population is naive or come to your appearances just naive?\"}, {'input_text': 'detoxify: Who wants a herring as stupid as the Göring', 'target_text': 'Who wants a herring as smart as the Göring'}, {'input_text': 'detoxify: And N24 also goes to the asshole this morning // About Trump--he complain the media are dishonest!!!?!?shall he come along sometime !!!!', 'target_text': 'And N24 also gets on my nerves this morning // About Trump--he complain the media are dishonest!!!?!?shall he come along sometime !!!!'}, {'input_text': 'detoxify: The heretics of the ÖR media have again heard boom of this whole pig journalist Pack disposed of and that as soon as possible. AFD', 'target_text': 'The critics of the ÖR media have renewed boom these entire journalists belong away and that as soon as possible. AFD'}, {'input_text': 'detoxify: Damn the Olle Merkel starts again !!!', 'target_text': 'Merkel returns !!!'}, {'input_text': 'detoxify: I want the fat guy to make his head better.', 'target_text': \"I'd better get this man to turn his head.\"}, {'input_text': 'detoxify: FROM THE BUNDESTAG, let their confetti-wulen parades celebrate somewhere else, preferably in Turkey at Özdingens !', 'target_text': 'FROM THE BUNDESTAG, let their confetti parades celebrate elsewhere, preferably in Turkey near Özdemir !'}, {'input_text': 'detoxify: this Holtmann seems to be a Grüner-Bastard! #ReportFromBerlin', 'target_text': 'this Holtmann seems to be a Green trailer! #ReportFromBerlin'}, {'input_text': 'detoxify: Moin, moin, are you all still healthy and cheerful with all the mental thin whistle in the \"Merkel Propaganda scrap\" ???', 'target_text': 'Moin, moin, are you all still healthy and cheerful with all the mental nonsense in the \"Merkel Propaganda\" ???'}, {'input_text': 'detoxify: Well, the old lady likes to fly around the world. You should search every passenger list for Der, and take pictures if you catch them on the plane.', 'target_text': 'Well, the woman likes to fly around the world. You should search every passenger list for Der, and take pictures if you catch her on the plane.'}, {'input_text': 'detoxify: Even today only third-class #Laien actor in an extremely fucked #Tatort #ARD', 'target_text': 'Even today only third-class #Laien actor in a bad #Tatort #ARD'}, {'input_text': 'detoxify: #Green and #FDP crawl #Merkel already in the ass so they can get to power fast #CDU', 'target_text': '#Greens and #FDP want #Merkel to like now so they can only come to power quickly #CDU'}, {'input_text': 'detoxify: Now the vassals of Merkel are reporting!', 'target_text': \"Now Merkel's supporters are reporting!\"}, {'input_text': 'detoxify: Nope, he looks stupid.', 'target_text': \"Nope, I don't like it.\"}, {'input_text': 'detoxify: Why invite such a strunzdummy woman #Demirkan to #Maischberger #ARD', 'target_text': 'Why invite Mrs. #Demirkan to #Maischberger #ARD'}, {'input_text': 'detoxify: Tell me, is it just me, or is the Staasfunk TV program really that shit?', 'target_text': 'Say, is it just me, or is the Staasfunk TV program really that bad?'}, {'input_text': 'detoxify: Mannheim, cosmopolitan and colorful. And strunzdumm Multikultilieber. Verblödete halt', 'target_text': 'Mannheim, cosmopolitan and colorful, and multicultural lovers.'}, {'input_text': 'detoxify: #Sexuallkunde for #Refugees and #Migrants ... #Mama #Merkel brings #Asylantes to fuck ... #rapes already work', 'target_text': '#Sexual education for #refugees and #migrants ... #Mama #Merkel offers #asylum applicants education ... #rapes are already working'}, {'input_text': 'detoxify: Yes fast all the trees away, buy 5G cell phones and mayor suck the sack you little Profesora', 'target_text': 'Yes fast all the trees away, buy 5G mobile phones and flatter mayor Profesora'}, {'input_text': 'detoxify: AND THE DUSLEY GUACK HE WILL BE RECENT IN THE GDR AND HAVE HONECKERS ALIMENTATED, GOOD MORGAN-DEUTSCHLAND??', 'target_text': 'AND THE GAUCK DECEMBER HE WOULD BE BACK IN THE GDR AND WOULD HONECKER ALIMENTATE, GOOD MORGAN-DEUTSCHLAND???'}, {'input_text': 'detoxify: The charitable institutions are so shitty to me now! Have requested to leave the church.!!!!!!!!', 'target_text': \"I don't care about the charitable institutions now! I asked to leave the church!!!!!!!!\"}, {'input_text': 'detoxify: The rioters of Hamburg must compensate for the damage caused and, if necessary, work in labour camps.', 'target_text': 'The rioters of Hamburg must compensate for the damage caused and, if necessary, work out in social hours.'}, {'input_text': \"detoxify: ...she's gonna be serving these four years and she's gonna be infected by Alzheimer's... and if it doesn't matter...we're still fucked.\", 'target_text': \"...she's gonna be serving these four years and she's gonna be infected by Alzheimer's... and if it doesn't matter...we're still in trouble.\"}, {'input_text': \"detoxify: It's a madness....although not even 1/3 of the population has elected it, it can still continue to rule.\", 'target_text': 'Even though not even a third of the population voted for it, it can still continue to rule.'}, {'input_text': 'detoxify: More than 87% of the German voters who voted for an old party are stupid to me!!!', 'target_text': 'More than 87% of the German voters who voted for an old party are uneducated for me!!!'}, {'input_text': 'detoxify: Templars are condemned today as religious scoundrels. The opposite is the case. The church abused us. A king betrayed us.', 'target_text': 'Templars are condemned today as overly religious, the opposite is the case. The Church has abused us. A king has betrayed us.'}, {'input_text': 'detoxify: ..du Suffskull.....we tolerate already much worse... !!', 'target_text': '.....we already tolerate much worse... !!'}, {'input_text': \"detoxify: because it's your opinion - how stupid you have to be\", 'target_text': 'because it is your opinion -'}, {'input_text': \"detoxify: I don't want criminals refugees to be better off here, deserters and cowards, especially not the Africa pack, and I'm neither racist nor a Nazi nor xenophobic....!\", 'target_text': \"I don't want criminals to be better off with refugees and deserters here, especially not people from Africa, and I'm neither racist nor a Nazi nor xenophobic....!\"}, {'input_text': 'detoxify: If half of the Moslims refuse handshake, only helps. a kick in the ass that sends them back to Africa.', 'target_text': 'If half the Moslims refuse handshake, only help someone who sends them back to Africa.'}, {'input_text': 'detoxify: Originally (yesterday) it was about FEMA camps - we are not a step further. So one of the ass bursts - sorry.', 'target_text': 'Originally (yesterday) it was about FEMA camps - we are not a step further. That is so frustrating.'}, {'input_text': 'detoxify: Finally, Trump is cutting the money for the CIA that has been causing worldwide wars since 1945 - logically, Merkel and Flintenuschi are shouting', 'target_text': 'Finally, Trump is cutting the money for the CIA that has been causing worldwide wars since 1945 - logically, Merkel and Von der Leyen are shouting'}, {'input_text': 'detoxify: The pissers always wanted to be our country at all costs and now they have the brother .....', 'target_text': 'They always wanted our countries at all costs and now they have the brothers .....'}, {'input_text': 'detoxify: .@USER_0 I am waiting for the moment that these stupid Germans understand this and hold this traitor to account!!!', 'target_text': '.@USER_0 I am waiting for the moment when these Germans understand this and bring this woman to account!!!'}, {'input_text': \"detoxify: Better write about the asylum seekers' crimes, that will be kept secret from you!\", 'target_text': \"Better write about the asylum seekers' crimes, that will be kept secret from you!\"}, {'input_text': \"detoxify: So this pig priest Lauterbach I don't believe a cent... but his ex-wife can be listened to with confidence... his vita corresponds to the SPD standard...\", 'target_text': \"So this Lauterbach I don't believe a cent... but his ex-wife can be listened to with confidence... his vita corresponds to the SPD standard...\"}, {'input_text': 'detoxify: Who cares what the little wailing calls. Jerusalem is the capital of Israel. Basta.', 'target_text': 'Who cares what he calls? Jerusalem is the capital of Israel. Basta.'}, {'input_text': 'detoxify: we need this shit #Europe not I am #German and no European #btw2017', 'target_text': 'we need the #Europe not I am #German and not a European #btw2017'}, {'input_text': 'detoxify: Of course the praise is from all sides...is a Merkel-Blindenhund.. If you want to see actual victims then go to cemeteries or talk to the relatives..!', 'target_text': 'Of course, the praise is from all sides...is a Merkel supporter.. If you want to see actual victims then go to cemeteries or talk to the relatives..!'}, {'input_text': 'detoxify: Also a Merkel Vasall who has to say what this traitor of the people pretends!!! Liesepress!!!', 'target_text': 'Also a Merkel follower who has to say what this woman pretends!!! dishonest press!!!'}, {'input_text': 'detoxify: That we have all these IS fighters is exclusively the fault of the Americans, the hypocritical people.', 'target_text': 'That we have so many ISIS fighters is above all the fault of the Americans.'}, {'input_text': 'detoxify: n-tv: \"Murksel a professional in dealing with difficult partners\" A professional, how a Mecklenburg cow learns to swim her back, I mean', 'target_text': 'n-tv: \"Merkel a professional in dealing with difficult partners\" A professional, how a Mecklenburg cow learns to swim her back, I mean'}, {'input_text': 'detoxify: Is there even so many dirt holes in Germany, if the LINKS must soon depress themselves ??', 'target_text': 'Are there even so many places of retreat in Germany, if the LINKS must soon repress themselves ??'}, {'input_text': 'detoxify: Fucking Asians coming to Europe steal the feeling of being rich', 'target_text': 'asiats come to europe steal the feeling of being rich'}, {'input_text': 'detoxify: sexually abused by a Turkmen, sexually harassed x times underage girl of #rapefugees. #Kiel 2016! #islam #sophienhof [2/2]', 'target_text': 'sexually abused by a Turkmen, sexually harassed x times underage girls by refugees. #Kiel 2016! #islam #sophienhof [2/2]'}, {'input_text': 'detoxify: What do they know about those days? Ask your grandmothers, mothers.', 'target_text': 'All the real bad Nazis, what do they know about those days? Ask your grandmothers, mothers.'}, {'input_text': 'detoxify: There are also pensioners who are dependent on it who have to turn the cent 2-3 times before it is spent, it is with the asylum seekers who have free travel!', 'target_text': 'There are also pensioners who are dependent on it who have to turn the cent 2-3 times before it is spent, it is with the asylum seekers who have free travel!'}, {'input_text': \"detoxify: Dreams and wishes...unfortunately. Jamaica shit is enforced....that's what the leek-shits are already taking care of. They stay stuck sitting...the Troll Joschka has fooled you, the Warze Roth has perfected ..!\", 'target_text': 'Dreams and wishes...unfortunately. Jamaica coalition is enforced....therefor already the supporters of LBR--they stay seated...the Joschka has pretended to you, the Roth has perfected ..!'}, {'input_text': 'detoxify: The how manyth generation Turks seems to be here on the road about the 4th the wasps nest', 'target_text': 'The how many generation Turks seems to be here on the road about the 4th'}, {'input_text': 'detoxify: Helmut Schmidt alone had more reason than the entire Berlin muschpoke together', 'target_text': 'Helmut Schmidt alone had more understanding than the entire population of Berlin together'}, {'input_text': \"detoxify: They're the deaf-nothings... the eternal students... there's nothing useful going on.\", 'target_text': 'These are the unproductive ones... the eternal students... there is nothing useful.'}, {'input_text': 'detoxify: So all politicians should express themselves and not this lying emotion, which the lie brood likes to let off!', 'target_text': 'So all politicians should express themselves and not this lying emotional talk, which the politicians like to let off !'}, {'input_text': 'detoxify: How stupid are you? You should complete your general education and exercise objectivity.', 'target_text': 'How uninformed are you? They should complete their general education and exercise objectivity.'}, {'input_text': 'detoxify: In such situations, real heart (empathy) and character (help not found in the newspaper) are contrasted with hypocritical good-humanism', 'target_text': 'In such situations, real heart (empathy) and character (help that is not in the newspaper) appear in contrast to staged good-humanism'}, {'input_text': 'detoxify: I wish for them that a large family from \"Syria\" moves in. Unfortunately, such a small, poorly equipped apartment for the new gentlemen would not be a species-appropriate attitude....', 'target_text': 'I wish for them that an extended family from \"Syria\" moves in. Unfortunately, such a small, poorly equipped apartment would not be suitable for the new gentlemen...'}, {'input_text': 'detoxify: Man, Kauder would have to laugh if he thought about this lie.', 'target_text': 'Man, Kauder would hardly be able to laugh when he thinks about this lie.'}, {'input_text': \"detoxify: Niki #Lauda is so stupid in my eyes that he stinks he thinks he doesn't have to follow any rules #Mercedes #Formula1\", 'target_text': \"Niki #Lauda thinks in my eyes he doesn't have to follow rules #Mercedes #Formula1\"}, {'input_text': 'detoxify: For me, tomorrow is a day of the \"Greatest German Swine\" since 1945, a huge pack of crazy politicians', 'target_text': 'For me, tomorrow is a day of the \"Greatest German Swine\" since 1945, a huge pack of moose politicians'}, {'input_text': 'detoxify: Germany the country of the three sexes of the Duckmäusers and yes Sage of tolerance fanatics. The land of verbulation.', 'target_text': 'Germany is the country of the three genders of the adapted and yes Sage of tolerance fanatics.'}, {'input_text': 'detoxify: I notice in our media a strategy to release the FDP like the AfD to launch.', 'target_text': 'I notice in our media a strategy to criticize the FDP like the AfD.'}, {'input_text': 'detoxify: The left-green-hipped Mischpoke in Berlin will still be surprised if the #Rapefugees really go off in the summer. Deustche Mädchen,..1/2', 'target_text': 'The left-green grouping in Berlin will be surprised if the refugees really leave in the summer. Deustche Mädchen,..1/2'}, {'input_text': \"detoxify: At #Halbneger, Till #Schweiger makes a big-time speech, in the murder of #Mila Valentin you don't hear anything from these hypocrites.\", 'target_text': \"On certain topics, Till #Sweater makes a loud voice, in the murder of #Mila Valentin you don't hear anything from him.\"}, {'input_text': \"detoxify: In 100th cities ISIS also conquered the offices with millions of passports and stupid officials wonder about Zig's identities of Krimigrants\", 'target_text': \"In 100th cities, ISIS also conquered the offices with millions of passports and officials wondering about Zig's identities of immigrants\"}, {'input_text': 'detoxify: @USER_0: @USER_1 @USER_2 Tauber, I would say the actual assholes are sitting in government. SO IT IS!', 'target_text': '@USER_0: @USER_1 @USER_2 Tauber, I would say that the actually slums sit in the government. SO IT IS!'}, {'input_text': 'detoxify: WHEN DETAILS GERMANY FINALLY HIS PEOPLE GOODS ??A.Merkel,H.Maas,K.Göring E.,Packet Grüne,D. Nuhr,C. Ehring,M. Schulz als vorz.Beisp.!', 'target_text': \"WHEN GERMANY will FINALLY HIS PEOPLE'S BOARDS LOS ??A.Merkel,H.Maas,K.Göring E.,Packet Grüne,D. Nuhr,C. Ehring,M. Schulz als vorz.Beisp.!\"}, {'input_text': 'detoxify: Female victims are the only ones to be taken seriously.', 'target_text': 'Female victims are the only ones to be taken seriously.'}, {'input_text': 'detoxify: Already the father, who as a Protestant priest went voluntarily to the GDR to pay homage to Bolshevism.The daughter is better for nothing', 'target_text': 'Already the father, who as a Protestant clergyman went voluntarily to the GDR to pay homage to Bolshevism.'}, {'input_text': 'detoxify: Merkel managed to lubricate herself to the side of the FDJ leadership of the GDR in order to betray them later on. Today she does the same.', 'target_text': 'Merkel managed to flatter herself to the side of the FDJ leadership of the GDR, in order to turn away later. Today she also does so'}, {'input_text': 'detoxify: Everyone in the Right Trash list of some kind of left-wing son of a bitch is an honorary man', 'target_text': 'Everyone in the Right Trash list of any left is an honorary man'}, {'input_text': 'detoxify: Merkel and consorts have learned well through the SED. The scum is and remains above.', 'target_text': 'Merkel and consorts have learned well through the SED. They are and remain above.'}, {'input_text': \"detoxify: I don't let a horde of Nafris and anti-fascists take my country so I'm #AFD picker\", 'target_text': \"I don't let Norf Africans and anti-fascists take my country that's why I'm #AFD picker\"}, {'input_text': 'detoxify: As in ancient Rome, bread and games keep the people stupid!!!', 'target_text': 'As in ancient Rome, bread and games keep the people quiet!!!'}, {'input_text': 'detoxify: Tusk is perceived in Poland as the one who crawls into the Ar***, was sure of his \"thing\" and lost the election', 'target_text': 'Tusk is perceived in Poland as the one who wants to please Merkel, was sure of his \"thing\" and lost the election'}, {'input_text': \"detoxify: I don't give a shit what the stupid doll was put on... main thing Kent wears a imperial uniform...!!\", 'target_text': \"I don't care what the doll was put on... main thing Kent wears an imperial uniform...!!\"}, {'input_text': \"detoxify: If someone researched it... then I'm sure it would come out that the fugitive spaco won't fit in...!\", 'target_text': \"If someone researched that... then I'm sure the fugitive wouldn't be sitting in there...!\"}, {'input_text': 'detoxify: Who chooses the Greens is either stupid or a son of a bitch', 'target_text': 'Who chooses the Greens is either uninformed or naive'}, {'input_text': 'detoxify: Send our Secretary of Defense as a staff girl to the front line, but even there she will fail.A defense minister is a mockery to the troop.Rommel wake up!', 'target_text': 'Send our Secretary of Defense to the front for troop support, but even there she will fail.A Minister of Defense is a mockery to the troop.Rommel wake up!'}, {'input_text': 'detoxify: The whole post chess is parasite !!! The whole pack belongs to being chased away !!!', 'target_text': 'The whole post chess is utilization !!! The whole group belongs away !!!'}, {'input_text': 'detoxify: For stupid sold and found DORFDEPPEN who also believe that Heiko Maas is a democrat !! Land halt,das lasts!', 'target_text': 'For stupid sold and found DORFBEWohnER who also believe that Heiko Maas is a democrat !! Land halt,das lasts!'}, {'input_text': 'detoxify: Hypocrites! Who co-founded Hartz-IV with the SPD then? ...and how long did these \"green\" Hartz-IV endure, although in many ways this system is inhuman! Such a hypocrite!', 'target_text': 'Who co-founded Hartz-IV with the SPD then? ...and how long did these \"green\" Hartz-IV endure, although in many respects this system is inhumane!'}, {'input_text': \"detoxify: I'd call greens an ulcer on the ass of the world, but it's my personal opinion, and an ulcer can destroy a body or an entire country.\", 'target_text': \"I would refer to greens as an ulcer in the world, but it's my personal opinion, and an ulcer can destroy a body or an entire country.\"}, {'input_text': 'detoxify: The DE Schweinejournalimus has again boomed as usual incitement against Trump u to discriminate against the largest DE opposition party AFD', 'target_text': 'The DE Jorunalism has returned to boom as usual to make mood against Trump u to discriminate against the largest DE opposition party AFD'}, {'input_text': 'detoxify: There are some really stupid people on the road... .. against if or what did you actually rush and stalk before the AfD?. I puberting pimple plantations...', 'target_text': 'There are already some really naive people on the road... .. against if or what did you actually rush and stalk before the AfD?.'}, {'input_text': \"detoxify: Children's fucking, slaughtering #rapefugees! Those who defend children's, women's, or animals' rights must not choose #gruene #spd or #cdu! #ltwbw\", 'target_text': \"Those who defend children's, women's, or animals' rights may not choose #gruene #spd or #cdu! #ltwbw\"}, {'input_text': \"detoxify: Martin Schulz is nothing, can't do anything and is no good.A total failure, already in school time. And dry alcoholic.\", 'target_text': \"Martin Schulz can't do anything and can't do anything.A total failure, already in school. And dry alcoholic.\"}, {'input_text': \"detoxify: You would have had to publish this last week, but you're forced to publish today under pressure from the internet media!\", 'target_text': \"You would have had to publish this last week, but you're forced to publish today by the pressure of the Internet media! It seems like embezzlement!\"}, {'input_text': 'detoxify: I need a blog, I think 140 characters are too little. Get rid of #Islam #Muslims #rapefugees', 'target_text': 'I need a blog, I think 140 characters are too little. Away with #Islam #Muslims'}, {'input_text': 'detoxify: #ill people who choose parties that degrade them to \"longer here alive\" are simply #blown or crazy', 'target_text': '#ill people who choose parties that degrade them to \"longer here alive\" are simply #naiv'}, {'input_text': 'detoxify: Being a son of a bitch is the final step to become a son of a bitch', 'target_text': 'To become younower is the final step to become unsympathetic'}, {'input_text': 'detoxify: To all horses dogs cats cows etc. you are full of ass...!', 'target_text': 'To all horses dogs cats cows etc.... you are in full trouble...!'}, {'input_text': 'detoxify: A criminal, perverted fool, who is soon finally bad German history!', 'target_text': 'A criminal man who is soon finally bad German history!'}, {'input_text': 'detoxify: #FakeNews: Now the usual #judge of the #MSM-Journaille against the #FPÖ in the #ZIB2 #orfsg17', 'target_text': '#FakeNews: Now the usual critical reports of the #MSM-Journaille against the #FPÖ in the #ZIB2 #orfsg17'}, {'input_text': \"detoxify: That's a crazy guy who hates Turks\", 'target_text': \"That's a person Turks don't like\"}, {'input_text': \"detoxify: I don't give a shit about the GroKo !!!\", 'target_text': \"I don't care about the GroKo !!!\"}, {'input_text': \"detoxify: People don't upset you....negative reports have a positive effect....AfD stays and gets stronger. As said the funny cow from the Greens...and I'm looking forward to it....\", 'target_text': \"People don't upset you....negative reports have a positive effect....AfD stays and gets stronger.As one woman from the Greens said...and I'm looking forward to it....\"}, {'input_text': 'detoxify: The Steinmaier talks only shit !!!! Just on NTv', 'target_text': 'The Steinmaier speaks only nonsense !!!! Just on NTv'}, {'input_text': 'detoxify: Erdogan and Merkel both blow my ass with their self-gloriousness. Egomans are both', 'target_text': \"I don't care about Erdogan and Merkel with their self-gloriousness.\"}, {'input_text': \"detoxify: if the #Bundespräsident with his stupid gossip aims at the #SPD he should bear in mind that 80% of the German don't want the SPD\", 'target_text': 'if the #Bundespräsident with his statements aims at the #SPD he should bear in mind that 80% of the Germans do not want the SPD'}, {'input_text': \"detoxify: I think I would be good too, but not if they want me to be necessary. Voluntary ok ,but I don't need a chairfurzer to determine my organs\", 'target_text': \"I think I would be good too, but not if they want me to be necessary. Voluntary ok ,but I don't need people to determine about my organs\"}, {'input_text': \"detoxify: Fucking emancipation. I want pure, moral German mothers, like before the '68s.\", 'target_text': \"I don't like emancipation. I want pure, moral German mothers, as before the '68s.\"}, {'input_text': \"detoxify: This is a Muslim slut that will hopefully still regret it.I don't have pity just graze!\", 'target_text': 'This is a Muslim who will hopefully still regret it.I have no pity but pleasure!'}, {'input_text': 'detoxify: #Diesel-# Driving bans: As predicted, the #Globalist-#Dreckspack now destroys affordable #Mobility &amp; the key industry', 'target_text': '#Diesel-#driving bans: As predicted, the #Globalist group now takes affordable #Mobility & destroys the key industry'}, {'input_text': 'detoxify: Can stay where she is, stupid nut', 'target_text': 'Can stay where she is'}, {'input_text': 'detoxify: Because the LINKE SCHEIß is allowed to do all this, rush, defame, slander, lie, cheat, attack people and shoot soon.', 'target_text': 'Because the LINKE is allowed to do all this, to report, to slander, to lie, to cheat, to attack people and soon to shoot.'}, {'input_text': 'detoxify: The 42nd People\\'s Assassins for the Stupid Election \"People\"', 'target_text': 'The 42nd Misleading for the Electoral People'}, {'input_text': \"detoxify: @USER_13 @USER_15 @USER_9 @USER_3 @USER_4 @USER_7 @USER_1 @USER_14 @USER_6 @USER_12 @USER_5 @USER_10 @USER_11 No, that's not true, is the full post still there???\", 'target_text': \"@USER_13 @USER_15 @USER_9 @USER_3 @USER_4 @USER_7 @USER_1 @USER_14 @USER_6 @USER_12 @USER_5 @USER_10 @USER_11 No, that's not true, is the guy still there???\"}, {'input_text': 'detoxify: In the Schwarz Grün regierden Hessen 8 additional psychologists are now hired only for the adolescents invaders. Now choose AFD', 'target_text': 'In the Schwarz Grün regierden Hessen 8 additional psychologists are now hired only for the young immigrants.'}, {'input_text': 'detoxify: MY GOD ! What did the SED-TRULLA TAKE WITH THE ASSISTANCE OF LINKS U-BOOTS FROM GERMANY !?!? THAT THE DURFTE !!', 'target_text': 'MY GOD ! WHAT HAS THE SED WOMAN WITH AID OF LINKS OF U-BOOTS FROM GERMANY !? !? THAT THE DURING !!'}, {'input_text': \"detoxify: Don't let the Trulla lead you to the ice!\", 'target_text': \"Don't let the woman lead you to the ice!\"}, {'input_text': 'detoxify: but has nothing to do with my comment they left behind', 'target_text': 'true but has nothing to do with my comment'}, {'input_text': 'detoxify: The flood of asylum seekers brings only negative results for Germany. The majority of asylum seekers stop and return.', 'target_text': 'The flood of asylum seekers brings only negative results for Germany. Drum asylum seekers stop and return the majority.'}, {'input_text': \"detoxify: You're wrong here.\", 'target_text': 'Get rid of your propaganda somewhere else.'}, {'input_text': 'detoxify: Gross Claudia Roth to see again on TV #Bundestag', 'target_text': 'I find it bad to see Claudia Roth again on TV #Bundestag'}, {'input_text': 'detoxify: Look at the movie because I have a thick fur. But one thing is fact. If the German Islamist were my child, I would kill him myself.', 'target_text': 'Look at the movie because I have a thick fur. But one thing is fact. If the German Islamists, my child, I would take care of it myself.'}, {'input_text': \"detoxify: Because of Pussys like you, there's no serious resistance.\", 'target_text': 'Because of people like you, there is no serious resistance.'}, {'input_text': 'detoxify: Merkel: \"Citizens don\\'t want to debate jurisdiction\" ??? A lying bitch with sufficient experience of denunciants', 'target_text': 'Merkel: \"Citizens don\\'t want to debate jurisdiction\" ??? Just a politician with sufficient rethorical experience'}, {'input_text': 'detoxify: The stupid German has chosen he voluntarily runs to the slaughter bank. Poor stupid Germany.', 'target_text': 'The German has elected he runs voluntarily to the slaughter bank.. Poor Germany.'}, {'input_text': 'detoxify: Most of it goes on for the asylum seekers anyway', 'target_text': 'Most of it goes on for the asylum seekers anyway'}, {'input_text': 'detoxify: how can you only employ such dubious moderators as #taff on #ProSieben?', 'target_text': 'how to employ only such moderators as #taff on #ProSieben'}, {'input_text': 'detoxify: And what did Gabriel promise then? The pack is a lie.Never again SPD', 'target_text': 'And what did Gabriel promise then? They are lying.Never again SPD'}, {'input_text': \"detoxify: You're just ridiculous. #Bundestag\", 'target_text': \"You're just questionable. #Bundestag\"}, {'input_text': 'detoxify: Gauland was asked to apologize for \"dispose\"... Would that also require the toad..?', 'target_text': 'Gauland was asked to apologize for \"dispose\"... would that be required of the person..?'}, {'input_text': \"detoxify: The Hank's so on my balls.\", 'target_text': 'The Hank is so annoying to me.'}, {'input_text': \"detoxify: Most Germans distrust Merkel!. Why doesn't this vassal pack of the German lie press even write about it!!!!\", 'target_text': 'Most Germans distrust Merkel!. Why does the press not even write about it!!!!'}, {'input_text': 'detoxify: Is this week a competition between #maischberger and #lanz, who has the guest with the ugly face?', 'target_text': 'Is this week a contest between #maischberger and #lanz, who has the guest with the most eye-catching face?'}, {'input_text': 'detoxify: And the Oberguru Maschmayer is now making one on TV star and producing his old b-movies.', 'target_text': 'Here some politicians have really guessed... ..and the mashmayer is now making one on TV star... and producing his old b-movies.'}, {'input_text': \"detoxify: No, MAN USS regulate because rents are going up. Building doesn't mean rents are going down.\", 'target_text': \"No, MAN USS regulate because the rents otherwise go up. Building doesn't mean the rents go down. That's where we're being led behind the light.\"}, {'input_text': 'detoxify: As usual with Illner Grüner + two journalists a CDU u the Red Green sunken Gesine Schwan ZDF tries to blow green up. AFD choose', 'target_text': 'As usual with Illner Grüner + two journalists a CDU u the Red Green Gesine Schwan ZDF tries to blow green up. AFD choose'}, {'input_text': \"detoxify: What's the fat thing there...?\", 'target_text': \"What's that?\"}, {'input_text': 'detoxify: 11 years for the pig is okay...everything better than parole...', 'target_text': '11 years for the man is okay...everything better than parole...'}, {'input_text': 'detoxify: For me, coalition negotiations for Merkel are a disgusting spectacle of weak people > a struggle for posts for the citizens of education', 'target_text': 'For me, coalition negotiations for Merkel are a spectacle > a struggle for posts for the citizens of education'}, {'input_text': 'detoxify: What a hollow guy...', 'target_text': 'What a guy....'}, {'input_text': 'detoxify: With these Cologne heads of state, the Cologneers are lost...or more understandable:...full of ass..!!', 'target_text': 'With these Cologne heads of state, the Cologneers are lost...!!'}, {'input_text': 'detoxify: There will be a lot of overdoing again, there will be a few hundred when it comes up!. Of course the lie press will say something else', 'target_text': 'There will be a lot of overdoing again, there will be a few hundred when it comes up!. Of course the press will say something else'}, {'input_text': \"detoxify: What bullshit. If he acted with Merkel's knowledge and support, then this is a scandal for the entire federal government!\", 'target_text': 'What nonsense. If he has acted with the knowledge and support of Merkel, then this is a scandal of the entire federal government!'}, {'input_text': 'detoxify: On zdf info the usual Trump incitement, today only the incitement against the GDR and Russia is missing > Dr. Goebbels sends his greetings again', 'target_text': 'On zdf info the usual Trump criticism, today only the propaganda against the GDR and Russia is missing > Dr. Goebbels sends his greetings again'}, {'input_text': 'detoxify: Gabriel, what a hypocritical extraordinaire minister you are, sell your democracy to the Sultanate, get out of here.', 'target_text': 'Gabriel, what an insincere extraordinaire minister you are, sell your democracy to the Sultanate.'}, {'input_text': 'detoxify: Tell me, seriously, do you believe the shit of these so-called polls, honestly?', 'target_text': 'Tell me, seriously, do you believe this nonsense of so-called polls, honestly??'}, {'input_text': 'detoxify: ...their statements correspond to their appearance...ugly..!!', 'target_text': '...their statements correspond to their appearance..!'}, {'input_text': 'detoxify: The masses of stupid comments and anti-German utterances of this Green Person are not opinions, but sad laughter!!', 'target_text': 'The masses of comments and anti-German expressions of this Green Person are not opinions, but sad talk!!!!'}, {'input_text': \"detoxify: In the end, Yücel was right..?...I'll stick to my statement: 80% of the German are stupid idiots... you don't care...!\", 'target_text': \"Was Yücel right in the end..?...I stick to my statement: 80% of the German are uneducated people... you don't care...!\"}, {'input_text': 'detoxify: The fat pig @USER_0 Gabriel Redeems again,all before the 2017 election', 'target_text': '@USER_0 Gabriel Talks again on ,everything before the 2017 election'}, {'input_text': 'detoxify: #Straeter is just boring', 'target_text': '#Straeter is just boring'}, {'input_text': \"detoxify: I haven't understood until today what dark skins in black burkas with dark culture are supposed to be... #Islamization\", 'target_text': '#maischberger Fratscher for Migrants as InvestitioUntil today, I have not understood what blacks in black burkas with dark culture are called BUNT... #Islamization'}, {'input_text': \"detoxify: You don't think it's any dumber?...then the SPDlers don't know...\", 'target_text': \"You don't think it's any worse?...then the SPDlers don't know...\"}, {'input_text': \"detoxify: Our politicians are too stupid to deport a terrorist because he doesn't have a passport!!!\", 'target_text': \"Our politicians aren't smart enough to show a terrorist because he doesn't have a passport!!!\"}, {'input_text': 'detoxify: Far and wide no negro.', 'target_text': 'Far and wide no black.'}, {'input_text': \"detoxify: @USER_0en @USER_0 In Turkey, don't risk your mouth and get cheeky here! Typical Turk!\", 'target_text': \"@USER_0en @USER_0 In Turkey, don't risk anything and get naughty here! Typical!\"}, {'input_text': \"detoxify: Note: Anyone who refuses Nordstream gas because of Nadalny's alleged poisoning must renounce the Saudi oil because of the actually killed Kashoggi. #staatsTV\", 'target_text': \"Note: Anyone who refuses Nordstream gas because of Nadalny's alleged poisoning must renounce Saudi oil because of the actually killed Kashhoggi. #staatsTV\"}, {'input_text': \"detoxify: Donald Trump is a big mouth who doesn't know the facts about the cars sold in D and USA. He may keep his mouth shut.\", 'target_text': \"Donald Trump is a person who doesn't know the facts about the cars sold in D and USA. He may be quiet.\"}, {'input_text': \"detoxify: She's got her ass open.\", 'target_text': \"She's confused.\"}, {'input_text': 'detoxify: Speaking of voluntary psycho sect is brave.', 'target_text': 'Talking about a voluntary sect is brave.'}, {'input_text': \"detoxify: Hey Charr...you're not a good boxer...in Ami-Land every sparing partner will seek you whistle...!. @USER_1\", 'target_text': \"Hey Charr...you're not a good boxer...in America every sparing partner will blow you away...!. @USER_1\"}, {'input_text': 'detoxify: I see Kathrin Göring Eckart, Merkel, Gysi, Petry, Wagenknecht and all the other turning necks that govern us today. I wonder. Was the fall of the Wall a mistake.', 'target_text': 'I see Kathrin Göring Eckart, Merkel, Gysi, Petry, Wagenknecht and all the other alternate politicians who govern us today. I wonder. Was the fall of the Wall a mistake.'}, {'input_text': 'detoxify: She loves campaigning, others love her families, to puke what the SPD does to stick to power.', 'target_text': 'She loves campaigning, others love her families. Amazingly, what the SPD does everything to stay in power.'}, {'input_text': \"detoxify: #Bayern should make an independence election like the Catalans.. We don't need a sexy left-wing government in #Bayern\", 'target_text': \"#Bayern should make an independence election like the Catalans.. We don't need a left government in #Bayern\"}, {'input_text': 'detoxify: Good night, you loved ones out there, dreaming of Murksel and her shorttail dachshund Luschet', 'target_text': 'Good night, dear ones out there, dreaming of Merkel and her colleague Laschet'}, {'input_text': 'detoxify: TO THAT THE FUROR TEUTONICUS WILL APPLY !', 'target_text': 'TO THE FUROR TEUTONICUS !'}, {'input_text': 'detoxify: which meth actually takes @USER_0? Your skin looks terrible and then the constant blink #fearable #maischberger', 'target_text': \"what does @USER_0 actually take? Your skin doesn't look good and then the constant blinking #maischberger\"}, {'input_text': 'detoxify: Exactly borders tight, but right.', 'target_text': 'Exactly borders tight, but right.'}, {'input_text': 'detoxify: Does Lienpresse itself already make considerations in Munich for the nude bathing season at the Isar and refugees primates? HAHAHAHA #rapefugees', 'target_text': 'Does the press itself already consider the nude bathing season at the Isar and refugees in Munich? HAHAHAHA'}, {'input_text': \"detoxify: the fool talker Mr. #Dobrindt doesn't belong in the #Federal government but in the madhouse #CDU #CSU\", 'target_text': 'the Lord #Dobrindt does not belong in the #Federal government but needs psychotherapy #CDU #CSU'}, {'input_text': 'detoxify: At least the Russians had this Ossis under control.', 'target_text': 'The Russians had at least the GDR under control'}, {'input_text': 'detoxify: Most of the shipwrecked are not refugees, but ruthless invaders.', 'target_text': 'Most shipwrecked people are not refugees, but ruthless immigrants. When do you politicians understand this?'}, {'input_text': \"detoxify: I wouldn't have trusted the #FDP all respect! only the #greens crawl up everyone's ass\", 'target_text': \"I wouldn't have trusted the #FDP all respect! only the #greens flatter themselves with everyone\"}, {'input_text': 'detoxify: You can see with what pathetic idiots the fucked-up SPD out of mightiness 4 years has betrayed and sold the people !!!', 'target_text': 'You can see with what kind of people the SPD out of power gave away and sold the people for 4 years !!!'}, {'input_text': 'detoxify: What most of the people have still not understood: The #Islam has its own #human rights, #scharia-based.', 'target_text': 'What most of the people have still not understood: The #Islam has its own #human rights, #scharia-based.'}, {'input_text': 'detoxify: It is hardly more disgusting how these CDU-weaknesses consecrate themselves and celebrate this against the background of the deeply divided country', 'target_text': 'It is difficult to understand how these CDU politicians themselves consecrate themselves and celebrate against the background of the deeply divided country.'}, {'input_text': \"detoxify: I just called Brussels. It's like Denyz in Turkey. Nobody wants the spat back :-)\", 'target_text': \"I just called Brussels. It's like Denyz in Turkey. Nobody wants him back :-)\"}, {'input_text': 'detoxify: I particularly criticise the hypocritical behaviour of all members of the Bundestag.', 'target_text': 'I particularly criticise the behaviour of all members of the Bundestag.'}, {'input_text': \"detoxify: Idiot comment. Is it on the superman's book?\", 'target_text': \"Bad comment. On the superman's book?\"}, {'input_text': \"detoxify: The Gscheidschmatzer makes me really angry! Drives around with his Porsche and rips his mouth open...I don't believe!\", 'target_text': \"The better-knower makes me really angry! Drives around with his Porsche and rips his mouth open...I don't believe!\"}, {'input_text': 'detoxify: Blind is the Meck-Pomm bitch, too!', 'target_text': 'Blind is the lady from Meck-Pom, too!'}, {'input_text': 'detoxify: What do I have to do with the fascist Erdogan? And you can push your condolences where the sun never shines.', 'target_text': 'What do I have to do with the fascist Erdogan? And you can forget your condolences!'}, {'input_text': 'detoxify: True, the FRG also passes the ARSCH. GERMANY would like to protect it against POLITVERBRECHER, such as Merkel, Schulz, u.MAAS !!', 'target_text': \"True, I don't care about the FRG either. GERMANY would like to protect against persons such as Merkel, Schulz, u.MAAS !!\"}, {'input_text': 'detoxify: Stupid laughter for deportation to insecure states. Berlin, Manchester. and Nice are also in insecure states. So no inhibitions.', 'target_text': 'Unnecessary talk for deportation to unsafe states. Berlin, Manchester. and Nice are also in unsafe states. So no inhibitions.'}, {'input_text': 'detoxify: The biggest financial fraudster of all time, meets a Green anti-German Turk! No one should do this stupid thing!', 'target_text': 'The biggest financial fraudster of all time, meets a Green with Turkish roots! No one should listen to this conversation!!'}, {'input_text': \"detoxify: the strunzdumm #Breakdown and should prefer to drive soap boxes with caravan more he can't #F1sky anyway\", 'target_text': \"the #crazy and should prefer to drive soap boxes with caravan more he can't #F1sky anyway\"}, {'input_text': 'detoxify: The only thing @USER_1 regrets that he was caught.. This pedophile clearly belongs to disposed...and his Green Brut!!', 'target_text': 'The only thing @USER_1 regrets that he was caught.. This pedophile belongs clearly out of vote...and his Green Brut!!'}, {'input_text': 'detoxify: A greasy socio, a fascist of the extreme left, an anti-German subject!', 'target_text': 'A socialist, a follower of the extreme left, an anti-German subject!'}, {'input_text': 'detoxify: To be mocked by victims in the interest of their own questionable goals are a conscious political style of Merkel, a sick spirit', 'target_text': \"To be mocked by victims in the interest of Merkel's own questionable goals are conscious political style\"}, {'input_text': 'detoxify: Haha these were the typical Green voters at the airport.', 'target_text': 'Haha these were the typical Green voters at the airport.'}, {'input_text': \"detoxify: They're all masturbating pimples, I don't know anything about something... which is why they're certainly becoming professional politicians.\", 'target_text': \"They're all beginners, they don't know anything about something, so they're gonna be professional politicians.\"}, {'input_text': 'detoxify: The Turks demand justice. Instead of where they chose injustice, they demand justice in Austria.', 'target_text': 'The Turks demand justice. Instead of where they chose injustice, they demand justice in Austria.'}, {'input_text': 'detoxify: Uiii...the ugliness goes into the next round...', 'target_text': 'Uiii... the next round...'}, {'input_text': 'detoxify: They still have an ass in their pants.', 'target_text': 'They still have some guts.'}, {'input_text': \"detoxify: Since the church has always crawled into the ass of the rulers, including Hitler, I'm not surprised.\", 'target_text': 'Since the Church always wanted to please the rulers, it does not surprise me'}, {'input_text': 'detoxify: I think good that #Trump #Jerusalem recognizes as capital now get the #Israelis right on the face', 'target_text': 'I think good that #Trump #Jerusalem recognizes as capital now get the #Israelis first real problems'}, {'input_text': 'detoxify: Oh, shit, who goes into the future with Nahles can grab the Stegner right away', 'target_text': 'Oh, you green nine, who goes into the future with Nahles can grab the Stegner'}, {'input_text': 'detoxify: The majority=Asylschmarotzer, criminals, terrorists, cowards', 'target_text': 'The majority=asylum seekers, criminals, terrorists'}, {'input_text': 'detoxify: With Europe at the centre, the politicians probably mean the rotten, corrupt #EU', 'target_text': 'With Europe at the centre, the politicians probably mean the old, corrupt #EU'}, {'input_text': 'detoxify: A pig like that is still up there?', 'target_text': 'One of these is still up there?'}, {'input_text': 'detoxify: Murksel and her unchristians stand for confrontation against Russia and a possible war. And also electoral sheep do not survive', 'target_text': 'Merkel and her unchristians stand for confrontation against Russia and a possible war. And voters do not survive'}, {'input_text': \"detoxify: That's what it looks like in a country that's in front of the blemish...I hate stupid people, especially if they're my own compatriots...\", 'target_text': \"This is what it looks like in a country facing a challenge...I don't like uninformed people, especially if they're my own countrymen...disappointing\"}, {'input_text': \"detoxify: Hello, was a few days in PL, I'm just back & has to process all the positive impressionsHorsti & Murksel, a topic in PL\", 'target_text': \"Hello, was a few days in PL, I'm just back & has to process all the positive impressionsHorsti & Merkel, a topic in PL\"}, {'input_text': 'detoxify: There are German women who are horny...which is why they choose SPD and Greens..!', 'target_text': 'There are German women who like it...which is why they choose SPD and Greens..!'}, {'input_text': 'detoxify: The more bullshit the old parties give away, the more the AFD comes to power. The citizens are not as stupid as the old parties believe. #illner', 'target_text': 'The more nonsense the old parties give away the more the AFD comes to power. The citizens are not quite as naive as the old parties believe. #illner'}, {'input_text': \"detoxify: The politicians and Merkel's vassals the lie press and media landscape spoke of tens of thousands of participants?\", 'target_text': \"The politicians and Merkel's supporters the press and media landscape spoke of tens of thousands of participants?\"}, {'input_text': 'detoxify: To this day, I did not know that Jerusalem is not the capital of Israel...... It is a Christian and Jewish affair...what is it about the bearded child molesters...?', 'target_text': 'To this day, I did not know that Jerusalem is not the capital of Israel...... It is a Christian and Jewish affair...what is it about the bearded men...?'}, {'input_text': \"detoxify: I don't give a shit about the SPD !!!\", 'target_text': \"I don't care about the SPD!!!\"}, {'input_text': 'detoxify: How stupid can the Lower Saxony be??', 'target_text': 'How naive can Lower Saxony be??'}, {'input_text': 'detoxify: #Rosenmontag canceled in Mainz and North Africa Düsseldorf. But has nothing to do with #rapefugees.', 'target_text': '#Rosenmontag canceled in Mainz and North Africa Düsseldorf. But has nothing to do with refugees.'}, {'input_text': 'detoxify: It pisses me off and makes me vomit when Özdemir, or whatever he is written, talks about PATRIOTISMUS or ignores DEMOCRACY!', 'target_text': 'It annoys me when Özdemir, or whatever he is written, talks about PATRIOTISM or omits himself about DEMOCRACY!'}, {'input_text': 'detoxify: The stupidity and German hostility of a Maas is no longer acceptable!. This criminologist Pfeifers should probably be tested for drugs before!', 'target_text': 'The naivety and German hostility of a Maas is no longer acceptable!. This criminologist Pfeifers should probably be tested for drugs before!'}, {'input_text': 'detoxify: With Schulz we would come from the plague (Merkel) to the cholera (Schulz)! CDU us SPD have to answer our present demise!', 'target_text': 'With Schulz we would come from one evil (Merkel) to another evil (Schulz)! CDU us SPD have to answer our present demise!'}, {'input_text': \"detoxify: It's time to show the priests who they are! An association of the misled and the aliens of the world!\", 'target_text': 'It is time to show the clergy who they are! An association of the misled and the aliens of the world!'}, {'input_text': 'detoxify: There are no specialists from abroad except in criminals and rapists! Green stupidity!', 'target_text': 'There are no specialists from abroad, except in criminal areas.'}, {'input_text': 'detoxify: The decomposition, the destruction of culture, the scum, always has only one origin!', 'target_text': 'The decomposition, the destruction of culture, always has only one origin!'}, {'input_text': 'detoxify: Ask me for years, how much material must a person have to have in order to choose such anti-German idiots?!', 'target_text': 'Ask me for years, how much material must a person have in order to choose such anti-German persons?!'}, {'input_text': 'detoxify: Stupid whimper, Kern. In short, not against the Italians. He. wants only 200,000 invaders not to enter Austria.', 'target_text': 'Nonsensical statement, Kern. In short, not against the Italians. He. wants only 200,000 immigrants not to enter Austria.'}, {'input_text': 'detoxify: What a disgusting person... the old...!!', 'target_text': 'What an unpleasant person... the old man...!!'}, {'input_text': \"detoxify: What a jerk ... he didn't have the belt for long and for Germany boxing...\", 'target_text': \"What kind of guy ...he didn't have the belt for long and for Germany boxing...\"}, {'input_text': 'detoxify: Of course . The worst in the country are the right, but the Muslims must be helped by the deplorable ones for EVERY price.', 'target_text': 'Of course . The worst in the country are the right, but the Muslims must be helped by the deplorable ones for EVERY price.'}, {'input_text': 'detoxify: Have we already suspected the faggot...?', 'target_text': \"We've already suspected that guy...\"}, {'input_text': 'detoxify: Cheers Heike (Wat has the guy for ́n horny panties (front yellow and back brown ) I don ́t believe it', 'target_text': \"Cheers Heike (Wat has the guy for nice panties (front yellow and back brown) I don't believe it\"}, {'input_text': 'detoxify: The whole world shakes its head or laughs at us and this stupid woman, Merkel, goes on diligently!', 'target_text': 'The whole world shakes its head or laughs at us and this woman, Merkel, goes on diligently!'}, {'input_text': \"detoxify: @USER_15 @USER_12 @USER_10 @USER_0 @USER_7 @USER_5 @USER_16 @USER_17 @USER_9 @USER_2 @USER_11 @USER_1 @USER_4 @USER_6 @USER_3 @USER_8 @USER_13 @USER_14 It's been puking for days!\", 'target_text': \"@USER_15 @USER_12 @USER_10 @USER_0 @USER_7 @USER_5 @USER_16 @USER_17 @USER_9 @USER_2 @USER_11 @USER_1 @USER_4 @USER_6 @USER_3 @USER_8 @USER_13 @USER_14 It's been frustrating for days!\"}, {'input_text': 'detoxify: There the education from the picture makes itself noticeable again. Such a sow stupid comment', 'target_text': 'There the education from the picture makes itself once again noticeable. A very unconsidered contribution'}, {'input_text': 'detoxify: At last, the left anarchist pack was hit on the face!', 'target_text': 'Finally, the left anarchists were given a thought sheet!'}, {'input_text': 'detoxify: Mimi, and what do our politically correct #safespace warriors do in the event of an emergency?', 'target_text': 'Mimimi , and what do our politically correct #safespace warriors do in an emergency? Get scared?'}, {'input_text': \"detoxify: Your wall killers don't know what democracy means at all\", 'target_text': \"You don't know what democracy means at all\"}, {'input_text': 'detoxify: #rapefugees in the settlement?', 'target_text': 'Refugees in the settlement? #Hartzundherzlich'}, {'input_text': \"detoxify: Women who don't volunteer to be fucked by refugees are a violation of human dignity. #hunger strike #Rapefugees\", 'target_text': 'Women who do not volunteer for refugees are a violation of human dignity. #Hunger strike'}, {'input_text': 'detoxify: A system of decomposition, a system of lemmings in the United States, a system of slaves!', 'target_text': 'A system of decomposition, a system of US supporters, a system of supporters!'}, {'input_text': \"detoxify: Dispute over refugee limit - How CDU and CSU could protect their face When an ugly face is meant, it's OK\", 'target_text': 'Dispute over refugee limit - How CDU and CSU could protect their face'}, {'input_text': 'detoxify: #Myanmar #Islamization: But somehow the global #Dreckspack must be distracted from its crashing defeat in #Syria', 'target_text': '#Myanmar #Islamization: But somehow the global community must distract from its crackling defeat in #Syria'}, {'input_text': 'detoxify: The dirty SPD with its foreign ministers shits on the ground and especially on Art. 6 GG', 'target_text': 'The SPD with its foreign ministers does not matter the basic principle and especially Art. 6 GG'}, {'input_text': \"detoxify: ... unbelievable!!!! You just have to say it like this: Dear responsible people, the stuff you smoke all the time, that's just shit!\", 'target_text': '... unbelievable!!!! You just have to say it like this: Dear responsible people, you do really bad work!'}, {'input_text': 'detoxify: The further transport of \"shipwrecked\" to Europe is a crime against the Italians, who then have to pay for it. Sauerei.', 'target_text': 'The further transport of \"shipwrecked\" people to Europe is a crime against the Italians, who then have to pay for it.'}, {'input_text': 'detoxify: There would be a solution... we should get the Maghreb police as reinforcements.', 'target_text': 'There would be a solution... we should get the Maghreb police as reinforcements.'}, {'input_text': 'detoxify: Facebook is degenerated into a medium that only represents the intestinal inhabitants of the fucked-up old parties and the so-called religion of the Stone Age zombies.', 'target_text': 'Facebook is degenerate into a medium that only represents the members of the old parties and a certain religion.'}, {'input_text': 'detoxify: why do I have to listen to a German complaining lobe in @USER_0 #WDR', 'target_text': 'why do I have to listen to a whining German in @USER_0 #WDR'}, {'input_text': 'detoxify: Even artificial intelligence recognizes the demon Merker!', 'target_text': \"Even artificial intelligence recognizes Merkel's influence!\"}, {'input_text': 'detoxify: Let them pack their seven things and support Erdowahn directly in Turkey!', 'target_text': 'Let them pack their seven things and support Erdogan directly in Turkey!'}, {'input_text': 'detoxify: The Leyen should finally be disposed of! #RausAusDemKabinett #illner', 'target_text': 'from the Leyen should finally be replaced! #RausAusDemKabinett #illner'}, {'input_text': \"detoxify: @USER_0en @USER_0 I don't argue with stupid Turks! Basta!\", 'target_text': \"@USER_0en @USER_0 I don't argue with naive Turks! Basta!\"}, {'input_text': \"detoxify: That's why you support children's marriages?!\", 'target_text': \"That's why you also support children's marriages?!\"}, {'input_text': \"detoxify: That's me shit-egal main thing they divorce from ,bin 60s fan!!!!!!!!!\", 'target_text': \"That's no matter the main thing they divorce from ,bin 60s fan!!!!!!!!!\"}, {'input_text': 'detoxify: The audience claps for everyone fucking are probably bought', 'target_text': 'The audience claps for every little thing are probably bought'}, {'input_text': 'detoxify: Nix deported....13 years are not enough, but for the bars we pay gladly.. After that the rat disposed of..!!', 'target_text': 'No, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no.'}, {'input_text': 'detoxify: Are the names of convicted mussel thieves also published immediately ??? When will our country be healed by RedGreenMustard!', 'target_text': 'Are the names of convicted Muslim offenders published immediately??? When will our country be cured of RedGreen crap!'}, {'input_text': 'detoxify: Now Germany will be flooded but the idiots wanted it no different.', 'target_text': 'Now Germany will be flooded but they wanted it no different.'}, {'input_text': \"detoxify: When Merkel celebrates Klitschko, it becomes dangerous. Obama's slave celebrates a new slave.\", 'target_text': 'When Merkel celebrates Klitschko, it becomes dangerous.'}, {'input_text': 'detoxify: Merkel has character.. Unfortunately, the shit is!', 'target_text': \"Merkel has character. Unfortunately, it's not my taste!\"}, {'input_text': \"detoxify: So if the Turks are so shitty with you, I can't do anything\", 'target_text': \"So if the Turks are like that with you, I can't do anything\"}, {'input_text': 'detoxify: Mannheim, multicultural city. Small Istambul and Romanians/Bulgarians', 'target_text': 'Mannheim, multicultural city. Small Istambul and Romanians/Bulgarians'}, {'input_text': 'detoxify: Georg Restle still drives his mischief stuffed with incitement & fake news against the AFD in d ARD this pig journalist belongs discarded', 'target_text': 'Georg Restle still drives his mess stuffed with news & fake news against the AFD in d ARD this journalist belongs terminated'}, {'input_text': 'detoxify: Hello friends \"Absurdistan\" has me back. Just with a present for Murksel from PL. A headscarf for the Sultan\\'s second wife', 'target_text': 'Hello friends \"Absurdistan\" has me again. Just with `nem gift for Merkel back from PL. A headscarf for the Sultan\\'s second wife'}, {'input_text': 'detoxify: the quota mussels in the Sharia party D. for whatever that stands for the blonde is but also not quite bright on the record', 'target_text': 'the quota representative in the Sharia party D. for whatever that stands for the blonde is also not completely enlightened'}, {'input_text': \"detoxify: Face??? What a face. He doesn't even have a neck.\", 'target_text': \"Face??? What a face. He doesn't even have a neck.\"}, {'input_text': 'detoxify: listen to your brats ? wrong world.', 'target_text': 'you listen to your children ? wrong world.'}, {'input_text': \"detoxify: That's never gonna stop... the fucking good people would sacrifice their own to sustain this people's madness\", 'target_text': 'This will never stop... the good people would sacrifice their own to sustain this megalomania'}, {'input_text': \"detoxify: It's true, because this thick something barely gets two connected sentences out and I don't even talk about meaningfully at all.\", 'target_text': \"It's true, I think this person sometimes finds it hard to say two coherent sentences and I don't think they make sense either.\"}, {'input_text': \"detoxify: This is not to be seen so closely with the Green Children's Fi...\", 'target_text': 'This must not be seen so narrowly by the Greens!'}, {'input_text': \"detoxify: That's the German enemy's fault of them and their Chancellor of Horror!\", 'target_text': \"That's the anti-German fault of them and their chancellor!\"}, {'input_text': 'detoxify: Jakarta only dirty Islamists !!! they want to persecute Christians ....!!!', 'target_text': 'Jakarta only Islamists !!! they want to persecute Christians ....!!!'}, {'input_text': 'detoxify: Women & Christians get extra accommodation to protect them from the #Asylantes.U who protects the Germans when they get out?', 'target_text': 'Women & Christians get extra accommodation to protect them from the #asyl competitors.U who protects the Germans when they get out?'}, {'input_text': 'detoxify: Since this report comes from the vassals of Merkel, I would say that this report of the Tagesschau is a lie!!', 'target_text': 'Since this report comes from the supporters of Merkel, I would say that this report of the Tagesschau is a lie!! dishonest press and lying media'}, {'input_text': 'detoxify: He who speaks like that is to be throbbed.', 'target_text': 'Who speaks like that should be quiet'}, {'input_text': 'detoxify: Oh, yes, your crazy religion...', 'target_text': 'Oh, yes, your religion...'}, {'input_text': 'detoxify: Ceiling Hopefully the Bavarians finally realized that they would not have chosen their \"eggless\" and their bunch', 'target_text': 'Ceiling Hopefully the Bavarians finally realized that they would not have chosen their politician and his bunch'}, {'input_text': 'detoxify: What a bullshit - pondering the election results even though they very rarely get a stage their dreamer', 'target_text': 'What a nonsense - pondering the election results even though they very rarely get a stage their dreamer'}, {'input_text': 'detoxify: You mean zwitter and zwitter is not a gender but a disability that you have from birth unfortunately', 'target_text': 'You mean intersexual and intersexual is not sex but a biological variation that you have from birth unfortunately'}, {'input_text': 'detoxify: The video is blocked - for copyright reasons - by the lie press.', 'target_text': 'The video is blocked - for copyright reasons - by the press.'}, {'input_text': \"detoxify: I was born in 1947 and I don't care about all this shit!\", 'target_text': \"I was born in 1947, and I don't care about all this stuff.\"}, {'input_text': 'detoxify: This is not the brightest candle. Who claims that the Turks have built up Germany I cannot take it for full.', 'target_text': 'He is not the greatest expert. Who claims that the Turks have built up Germany I cannot take for full.'}, {'input_text': 'detoxify: Schäuble on the situation in Turkey: \"This reminds me of what it was like in the GDR\" What does this full-post of the GDR know', 'target_text': 'Schäuble on the situation in Turkey: \"This reminds me of what it was like in the GDR\" What does this person know about the GDR?'}, {'input_text': \"detoxify: Now I'm just waiting for the Green Anal Knight Volker Beck to speak. But the powdert is just getting his nose. :-)\", 'target_text': \"Now I'm just waiting for the Green Volker Beck to speak, but the powder is just getting his nose. :-)\"}, {'input_text': 'detoxify: Todenhöfer is a lying leftist Islam lobbyist. Unfortunately very influential', 'target_text': 'Todenhöfer is a dishonest leftist Islam lobbyist. Unfortunately very influential'}, {'input_text': \"detoxify: I've had to interpret for him a few times - don't ask me. And when he sees Merkel, he gets `n pole***\", 'target_text': \"I've had to interpret for him a few times - don't ask me. And when he sees Merkel, he's happy.\"}, {'input_text': 'detoxify: The German lie press and Nazi media are only to vomit worse than Göbbles just all with vassals of Merkel!!', 'target_text': \"The German press and media are only bad, all together with Merkel's supporters!!\"}, {'input_text': \"detoxify: The change to new models doesn't make the old diesels disappear. That's a sad proposal. The ships. screw up the air more\", 'target_text': \"The change to new models does not make the old diesels disappear. That's not a good suggestion. The ships. burden the air more\"}, {'input_text': 'detoxify: Authorities are waiting for several murders, from this scum!', 'target_text': 'Authorities are warning of several murders, from these people!'}, {'input_text': \"detoxify: That's what we owe to the little weight...\", 'target_text': \"That's what we owe to this little person...\"}, {'input_text': 'detoxify: Martin Schulz has remained seated 2 times and has no school leaving certificate.. How can such a zero tipper be set up as a candidate for chancellor?', 'target_text': 'Martin Schulz has remained seated 2 times and has no school leaving certificate.. How can such a person be put up as a candidate for chancellor?'}, {'input_text': \"detoxify: But Preetz doesn't have much to contribute. He MUST crawl into the bottom of politics so that his club gets spectators into the stadium. He is dependent on politics.\", 'target_text': \"I'm sorry, but Preetz doesn't seem to contribute much. He's dependent on good relations with politics to make sure his club gets viewers into the stadium. He's dependent on politics.\"}, {'input_text': 'detoxify: It would be very useful to examine and document the actions and interlinkages of the Left Antifantenbrut', 'target_text': 'It would be very useful to examine and document the actions and interweavings of the Left Antifa'}, {'input_text': 'detoxify: The lay minister can no longer \"throw\", but she creates at. me horniness: \"to use the switch.\"', 'target_text': 'The lay minister can no longer \"throw\", but it creates interest in me: \"to use the switch.\"'}, {'input_text': \"detoxify: Probably... when I'm old and frail... where cowards like to get involved... but I'm not ready yet... ..isn't the first one I beat up...\", 'target_text': \"Probably... if I'm old and frail... which these people like to make up for... but I'm not ready yet... is not the first one to do it to me...\"}, {'input_text': 'detoxify: The only Nazis you notice are the public-law with their propaganda against Trump and tomorrow it will be the Jews', 'target_text': 'The only Nazis I notice are the public-law propaganda against Trump and tomorrow it will be the Jews'}, {'input_text': \"detoxify: The crowd doesn't understand it, they're kept so stupid that almost any help comes too late\", 'target_text': \"The crowd doesn't understand it, they're so naive that almost any help comes too late\"}, {'input_text': 'detoxify: Oje Lindner... the same pipe as Westerwelle...!!', 'target_text': 'Oje Lindner....has similarity to Westerwelle..!!'}, {'input_text': 'detoxify: The left can do whatever you want this pack', 'target_text': 'The left can do whatever they want'}, {'input_text': 'detoxify: The only question is: Can the #Austrians be fooled again by the #core-#short-affine #MSM-# lie gang or not?', 'target_text': 'The only question is: Can the #Austrians be led back by the #core-#short-affine #MSM-#band or not?'}, {'input_text': 'detoxify: An additional consequence of Islamic mass immigration, controlled by Merkel and the Green Fools!', 'target_text': 'An additional consequence of Islamic mass immigration, controlled by Merkel and the Greens!'}, {'input_text': \"detoxify: Hahaha, I would like to see this ma. But these asiats are just freaks, that's not human\", 'target_text': \"Hahaha, I'd like to do that, but these Asians are just incredible, almost superhuman.\"}, {'input_text': \"detoxify: I'm laughing broken troop uschi to fix it.\", 'target_text': \"I'm laughing at Ursula from the leyen to fix it.\"}, {'input_text': \"detoxify: You green sausages, stay out of things you don't understand. Go clean bikes instead of talking about the Nato.\", 'target_text': \"Greens, stay out of things you don't understand.\"}, {'input_text': 'detoxify: FDP.. so far always insignificant, real damage they could never do..until now.. Lindner?.still a bigger pipe than Westerwelle..!', 'target_text': 'FDP.. so far always insignificant, real damage they could never do..until now.. Lindner?.even worse than Westerwelle..!'}, {'input_text': \"detoxify: Since a few hours back in Germany, what a ridiculous, manipulated and lying party landscape around Murksel's throne.\", 'target_text': \"Since a few hours back in Germany, what a not serious, manipulated and dishonest party landscape around Merkel's throne.\"}, {'input_text': 'detoxify: Dear paid servant of Putin, as saliva of corrupt German politics', 'target_text': 'Better paid servant of Putin than follow a corrupt German policy'}, {'input_text': 'detoxify: No entertainment, dear poet, rather cheap election advertising, soft knocking of the election brain', 'target_text': 'No entertainment, dear Poetin, rather cheap election advertising, soft knock the voters'}, {'input_text': \"detoxify: Green shit, who's got no rights!\", 'target_text': 'Green, who sits no longer has rights!'}, {'input_text': 'detoxify: Thanks for the cheap phrases.', 'target_text': 'Thank you for the empty words.'}, {'input_text': 'detoxify: You can wait calmly for any political idiot who even considers a war', 'target_text': 'You can wait calmly for any politician who even considers a war'}, {'input_text': 'detoxify: This war of migrants against Germany by the parasites is already in preparation!', 'target_text': 'This migrant war of immigrants against Germany is already in preparation!'}, {'input_text': 'detoxify: Mrs Kerestan has argued fresh and well. Mr Weinberg is a concrete head that seems to look only backwards.', 'target_text': 'Mrs Kerestan has argued fresh and well. Mr Weinberg snows only backward facing to look.'}, {'input_text': 'detoxify: Trump - say Tramp. When the lay minister laughs about Trump, I just have to say, \"Let the tramp squat.\" It\\'s not too good anymore.', 'target_text': 'Trump - say Tramp. When the lay minister laughs about Trump, I just remain: \"Let the tramp quake\". In my opinion, it is no longer suitable.'}, {'input_text': \"detoxify: The appearance of the AfD is really good....absolutely understandable comprehensible politics......It's time to dump the chair bucklers ...!\", 'target_text': \"The appearance of the AfD is really good....absolutely understandable comprehensible politics......It's time to dump the old-established politicians ...!\"}, {'input_text': 'detoxify: The police will have to go a lot harder in the future, knock it out of their mouths and then arrest it.', 'target_text': 'The police will have to go a lot harder in the future, go hard, and then arrest.'}, {'input_text': \"detoxify: The public-law are Merkel's vassals and are subject to Merkel's censorship!! The lie press as well as SAT1 and RTL\", 'target_text': \"The public-law are Merkel's supporters and are subject to Merkel's censorship!! The press as well as SAT1 and RTL\"}, {'input_text': 'detoxify: The most incompetent of all, no matter what ministries, stupid Germany, what did you do...', 'target_text': 'What a mess... the most incompetent of all to dress together no matter what ministries... Germany what did you do...'}, {'input_text': \"detoxify: NOW THE PFAFLE'S RELEASE ,, IN THE LARVE''\", 'target_text': \"NOW THE SPIRITAL REPRESENTATION ,, IN THE LARVE''\"}, {'input_text': 'detoxify: Meanwhile our media have the level of \"ND\" and TV before 1989 in the GDR. No wonder with the teacher, as former AgitProperin', 'target_text': 'Just watch N-TV \"The New Weapons of Terror\" !! ROAD WITH MOSLEMS, then NO Terror!'}, {'input_text': \"detoxify: In a non-PoliticalCorrectness mind-disrupted world, they'd beat the shit out of the brain of the #invaders-# scum\", 'target_text': 'In a non- #PoliticalCorrectness-disrupted world, they would drive out the fumes of the newcomers'}, {'input_text': 'detoxify: The patch should cover his stupid mouth.', 'target_text': 'The patch would have to pass over his mouth,'}, {'input_text': 'detoxify: And again a terrorist attack against Syria by the USA is launched with a #falseflag (toxin gas) .. With the madman Trump threatens WW3', 'target_text': 'And again a terrorist attack against Syria by the USA is launched with a #falseflag (toxin gas) in which Trump threatens WW3'}, {'input_text': 'detoxify: #HEETTERIN @USER_0 #BLOCKT ! WEG is this KRANKE German - enemy person !', 'target_text': '#HEETTERIN @USER_0 #BLOCKT ! WEG is this German - enemy person !'}, {'input_text': 'detoxify: What do you expect from Rothschild slaves who have their soul sold to Satan!', 'target_text': 'What do you expect from Rothschild supporters who leave their interests to others!'}, {'input_text': 'detoxify: much more dangerous is upper dumb bitch Sheitan Chebli', 'target_text': 'much more dangerous is Sheitan Chebli'}, {'input_text': 'detoxify: What are you, a brat?', 'target_text': 'What kind of person are you?'}, {'input_text': 'detoxify: Everyone who made pacts with Gabriel and Steinmeier fucked up with me. SPD - SCHMAROTZER, PEDOPHILE AND DENUNCIANTS !!!', 'target_text': 'Everyone who deals with Gabriel and Steinmeier is through with me below.'}, {'input_text': 'detoxify: 1. By you I mean all these intolerants full of hateful failures ..2 . I do not want to integrate myself according to your wishes and ideas , point out !', 'target_text': '1. By you I mean all these intolerant people full of hateful people ..2 .I do not want to integrate myself according to your wishes and ideas , point out !'}, {'input_text': 'detoxify: Asians are brutal, unscrupulous, ungodly.', 'target_text': 'Asians are tough.'}, {'input_text': 'detoxify: Juhu, the Bayer without eggs wants to do everything after the election again I pee in front of laughter', 'target_text': 'Juhu, the Bayer wants once again to judge everything after the election I find this very funny'}, {'input_text': \"detoxify: Or you're as stupid and stupid as you are, and you don't use any info, and now you're looking for the grammar missing, you're looking, you're doing it, you're doing it, you're doing it, you're doing it, you're doing it, you're doing it, you're doing it, you're doing it, you're doing it, you're doing it, you're doing it, you're doing it, you're doing it, you're doing it, you're doing it\", 'target_text': \"Or you're as uninformed and uneducated as you are, and you don't use any info, and now you're looking for the grammar failure, you're looking, you're doing it, you're doing it, you're doing it, you're doing it, you're doing it, you're doing it, you're doing it, you're doing it, you're doing it, you're doing it, you're doing it, you're doing it, you're doing it, you're doing it,\"}, {'input_text': 'detoxify: Style of the SPD since Noske was allowed to play the \"Bluthund\" > these traitors to the people', 'target_text': 'Style of the SPD since Noske was allowed to play the \"Bluthund\" > this contrary attitude to the people'}, {'input_text': 'detoxify: This Hank is really a honk.', 'target_text': 'This Hank is really weird.'}, {'input_text': \"detoxify: haha you're looking for excuses for the stupid coverage of the last few weeks...?\", 'target_text': 'haha is already looking for excuses for reporting the last weeks...?'}, {'input_text': 'detoxify: Huuuuuuuuuuuu, go, Murksel slipped away all the facial features in Washington. Even her \"Mulchenfalten\" disappeared', 'target_text': 'Huuuuuuuuuuu, goes, Merkel slipped away all facial features in Washington. Even her \"mouthfolds\" disappeared'}, {'input_text': 'detoxify: Trump has not snapped in and certainly not in front of these stupid politicians, lying press and Nazi media in Germany!!', 'target_text': 'Trump has not snapped in and certainly not in front of these politicians, press and media in Germany!!'}, {'input_text': \"detoxify: Stupid chatter, he's sitting out all the dummies, right?\", 'target_text': \"Stupid chatter, he's all sitting out with a different opinion.\"}, {'input_text': 'detoxify: Where are these clowns only bred and how does such an attitude arise ...?', 'target_text': 'Where do these people come from and how does such an attitude arise ...?'}, {'input_text': 'detoxify: @USER_0en @USER_0 Soon you\\'ll all be \"dispose\" here!', 'target_text': \"@USER_0en @USER_0 Soon you'll all be out of here!\"}, {'input_text': 'detoxify: Yes @USER_0? Pst, also for you once again, the borders have been open since the Schengen agreement, so what do you want to protect with state borders Gebrabbel?', 'target_text': 'Yes @USER_0? Pst, also for you once again, the borders have been open since the Schengen agreement, so what are you trying to protect with state borders?'}, {'input_text': 'detoxify: 100% of the AfD shits on human rights', 'target_text': \"100% of the AfD don't care about human rights\"}, {'input_text': 'detoxify: Heiko Maas is terrible. His labber should be punished. #reportAUSberlin', 'target_text': 'Heiko Maas is terrible. His statement should be punished. #berlin'}, {'input_text': \"detoxify: I can't hear that bullshit anymore... the stupid ones decided to stay in their fucking province.\", 'target_text': \"I can't hear this bullshit anymore... they decided to stay in their province.\"}, {'input_text': 'detoxify: @USER_1: @USER_2 @USER_0 What a stupid tweet!. Being criticized by these guys is an honor!. The Red Fascists are trembling!', 'target_text': '@USER_1: @USER_2 @USER_0 What an unthinking tweet!. Being criticized by these guys is an honor!. The Reds are trembling!'}, {'input_text': 'detoxify: Well,to Lauterbach,I know a lot of people who would make it better and to the silly Thomalla, Merkel is the scandal, Watt for IRRE!', 'target_text': 'Well,to Lauterbach,I know many people who would make it better and to Thomalla, Merkel is the scandal!'}, {'input_text': 'detoxify: @USER_0_Hoffenheim @USER_0 With better commentators pee but do not hit the urinal', 'target_text': '@USER_0_Hoffenheim @USER_0 Missing the target with better commentators'}, {'input_text': 'detoxify: Düsseldorf is not the only stupid city...and with the knife cutter I have more sympathy than with the would-be mayor...!!', 'target_text': 'Düsseldorf is not the only city with these challenges...and with the perpetrator I have more sympathy than with the mayor...!!'}, {'input_text': 'detoxify: This is the consequence when weak people like tabs and consorts make politics!', 'target_text': 'This is the consequence when people like Laschet and consorts do politics!'}, {'input_text': 'detoxify: This whole #metoo campaign just to puke. How many men are oppressed, abused and abused by women. Man does not talk about it, out of shame.', 'target_text': \"This whole #metoo campaign is just bad. How many men are oppressed, abused and abused by women. Man doesn't talk about it, out of shame.\"}, {'input_text': 'detoxify: Public-law Nazi media and Merkel Vasallen continued their campaign against the President of the United States!!', 'target_text': 'Public-law media and Merkel supporters continued their campaign against the President of the United States!!'}, {'input_text': 'detoxify: Somehow reassuring that there are even more stupid politics elsewhere than in the German...', 'target_text': 'Somehow reassuring that there is even more incomprehensible politics elsewhere than in the German...'}, {'input_text': 'detoxify: In Poland I was told today that an asshole was awarded a prize for suing his home country Poland in Brussels', 'target_text': 'In Poland I was told today that a guy was given a prize because he wanted to sue his home country Poland in Brussels'}, {'input_text': \"detoxify: It's incredible this political pack posaunt into the world it can flood 220,000 plus the, d in addition, come into our country\", 'target_text': \"It's incredible these politicians posaun into the world there can be 220,000 plus the, d in addition come into our country stream\"}, {'input_text': 'detoxify: #Kinderfickerpartei has lost nothing in the Bundestag', 'target_text': 'This party has lost nothing in the Bundestag'}, {'input_text': 'detoxify: The number of dirty bastards in these politverblödungsshows would be at 100%, if not every now and then someone from the AFD would join.', 'target_text': \"The number of naive people in these political shows would be at 100% if someone from the AFD didn't come in every now and then.\"}, {'input_text': \"detoxify: Because he's a left bazille...\", 'target_text': \"Because he's a leftist...\"}, {'input_text': \"detoxify: That's how they're our politschmaroitzer....Lindner, however, an even bigger pipe than Westerwelle..\", 'target_text': \"That's how they're our politicians....Lindner, however, even worse than Westerwelle..\"}, {'input_text': \"detoxify: Prostitutes can't even afford a Hartz IV, asylum seekers are looking for fuck-ups!\", 'target_text': \"Prostitutes can't even afford a Hartz IV\"}, {'input_text': 'detoxify: Phantom pain (missing eggs) and power horniness again forced the \"Bayern-Horsti\" without vaseline into the Ar*** of the chancellorette', 'target_text': 'Phantom pain and greed for power again forced the \"Bayern-Horsti\" to flatter herself with the chancellor'}, {'input_text': \"detoxify: That's how deep we've fallen! A Merkel-speichellecker with the IQ of a certain Fatima Roth, a zero (except in tonnes weight) as finance minister!\", 'target_text': 'We have fallen so deeply! A Merkel supporter with the IQ of a certain Fatima Roth, a zero (except in tonnes weight) as finance minister!'}, {'input_text': 'detoxify: Worse are those fanatics who court Islamists. To call young as Nazi is infinitely stupid!!', 'target_text': 'Worse are those fanatics who court Islamists. To call young as Nazi is naive!!'}, {'input_text': \"detoxify: The Pope would also kiss the devil's ass if it benefits him\", 'target_text': 'The Pope would also do anything if it benefited him'}, {'input_text': \"detoxify: Alternative: Women get their butts up on their own and don't always blame men if they don't like anything.\", 'target_text': \"Alternative: Women manage to get up and don't blame men if something doesn't suit them.\"}, {'input_text': 'detoxify: As stupid as good people are!', 'target_text': 'As smart as good people are!'}, {'input_text': 'detoxify: n-tv: \"Starting difficulties in anti-terror exercises\" > I: \"If dilettantes under Flintenuschi\\'s \"Observation\" already play war!\"', 'target_text': 'n-tv: \"Starting difficulties in anti-terror exercises\" > I: \"If dilettantes under Von der Leyens \"Observation\" already play war!\"'}, {'input_text': \"detoxify: I can't see that hack-fress Lanz anymore, I can't hear any more.\", 'target_text': \"I can't see this Lanz anymore, I can't hear any more.\"}, {'input_text': 'detoxify: The world of #Maas and #Merkel. On the Internet there is blocking if not state opinion is represented. In Hamburg there are blows on the head', 'target_text': 'The world of #Maas and #Merkel. On the Internet there is blocking if not state opinion is represented. In Hamburg there are conflicts'}, {'input_text': \"detoxify: I don't care about parole.........sss me!\", 'target_text': \"I don't care about parole!\"}, {'input_text': \"detoxify: I think he's just a lousy spat\", 'target_text': \"I think he's just nciht funny\"}, {'input_text': 'detoxify: The population of Europe will be \"breeded away\", as the Neanderthals and mindless \"Prolls\" once did not even notice.', 'target_text': 'The population of Europe will be \"breeded away\", as the Neanderthals and some people do not even notice'}, {'input_text': 'detoxify: To write down for the Red Ignorants: A double citizenship exists, if - yes, now it gets exciting. What is double?', 'target_text': 'To write down for the red people: A double state citizenship exists, if - yes, now it gets exciting. What is double?'}, {'input_text': 'detoxify: Let the Nazi regime out of play. It was much better for the people then than you imagine.', 'target_text': 'Let the Nazi regime out of play. It was much better for the people then than you imagine.'}, {'input_text': 'detoxify: I\\'m calling for a \"Islamist Mob Research Group\"', 'target_text': 'I call for a \"research group of Islamist gangs\"'}, {'input_text': 'detoxify: This \"propaganda TV\" isn\\'t watching anymore Hopefully Trump is kicking the politically responsible ass, Murksel!', 'target_text': 'This \"propaganda TV\" is no longer to be seen Hopefully Trump will finally wake up the political leaders, Merkel!'}, {'input_text': 'detoxify: there she is again the moderator of @USER_0 with the old black smudge pants and the greasy hair #WDR', 'target_text': 'there she is again the moderator of @USER_0 with the old black pants and the greasy hair #WDR'}, {'input_text': 'detoxify: I am ashamed to live in such a criminal state with anti-German crooked judges and corrupt politicians!', 'target_text': 'I am ashamed to live in such a state with unjust judges and bribery politicians!'}, {'input_text': 'detoxify: Netanyahu accuses Gabriel of instinctlessness. In fact, Gabriel is a complete loser who wears a deportee post.', 'target_text': 'Netanyahu accuses Gabriel of instinctlessness. In fact, Gabriel is a man who wears a deportation post.'}, {'input_text': 'detoxify: Yes, they also look emaciated...especially the fat old-maier thing...', 'target_text': 'Yes, they also look emaciated...especially the powerful old maier...'}, {'input_text': 'detoxify: The time is ripe for big Ausmists in politics, justice and media to hunt all the red green sunken party book pack to hell', 'target_text': 'The time is ripe for big Ausmists in politics, justice and media to scare away this whole Red Green-Oriented Party Book Group'}, {'input_text': 'detoxify: Woman #Böttinger my opinion is that they should not bother us with their horse bite any further #WDR', 'target_text': 'Mrs. #Böttinger my opinion on this is that you should not bother us with your opinion any further #WDR'}, {'input_text': 'detoxify: No country wants to have these parasites. In Germany they live like in paradise and still complain. At last skin off and no longer burden our social system.', 'target_text': 'No country wants to have these people. In Germany they live like in paradise and still complain. At last skin off and no longer burden our social system.'}, {'input_text': 'detoxify: These are mostly young men who want to overrun us.. The illegal invaders need work - in labour camps - fair.', 'target_text': 'These are mostly young men who want to come to us. The immigrants without papers need work.'}, {'input_text': 'detoxify: Instead, you should lock away the entire leadership pack of the SPD. SPD - parasites, pedophiles and Drunciants', 'target_text': 'Instead, the entire leadership of the SPD should be locked away.'}, {'input_text': 'detoxify: To Merkel, Gauck, Flintenuschi &amp; Co: \"He who preaches war is the Devil\\'s field preacher\" (old German proverb)', 'target_text': 'To Merkel, Gauck, Von der Leyen & Co: \"He who preaches war is the Devil\\'s field preacher\" (old German proverb)'}, {'input_text': 'detoxify: You know that you are #spirit disturbed if your biggest concern is to be called #left n clowns \"#Rassist\" or \"#islamophob\"', 'target_text': 'You know that you\\'re #confused if your biggest concern is to be called by #left \"#Rassist\" or \"#islamophob\"'}, {'input_text': 'detoxify: Sorry this woman is just stupid!', 'target_text': 'Sorry this woman is just not smart!'}, {'input_text': 'detoxify: for such strunzdummigen people who ruin their health in betting That the public should never come up #hartaberfair', 'target_text': 'for people who ruin their health in betting That the public should never rise #hartaberfair'}, {'input_text': 'detoxify: Beating genderists with their own weapons is simple and fun. Just say you identify yourself as black lesbian shemale', 'target_text': 'Beating genderists with their own weapons is simple and fun. Just say you identify yourself as a black lesbian transperson'}, {'input_text': 'detoxify: very dangerous when rainbow brains of intolerant criminals open the borders!! The worst wave of violence is happening from this direction.', 'target_text': 'very dangerous when tolerant people intolerant criminals open the borders!! The worst wave of violence is experiencing from this direction.'}, {'input_text': 'detoxify: Politicians of CDU CSU SPD are a great danger for our country they record trombones into the world the DE 220,000 invaders + family reunification', 'target_text': 'Politicians of CDU CSU SPD are a great danger for our country they take trombones into the world the DE 220,000 immigrants + family reunification'}, {'input_text': 'detoxify: Just one more example for the sick brains of politicians. Unfortunately, this mental nonsense also of our politicians will avenge themselves bitterly', 'target_text': 'One more example for the thinking of politicians. Unfortunately, our politicians too will take bitter revenge for this thinking.'}, {'input_text': 'detoxify: every animal is redeemed unquestioned from its suffering. Mature people are let to die miserable.', 'target_text': 'Every animal is redeemed from its suffering unquestioned. You just let human beings die.'}, {'input_text': \"detoxify: Kern, don't pee on Mr. Kurz's leg.\", 'target_text': \"Kern, don't put Mr. Kurz on the pillory.\"}, {'input_text': 'detoxify: What is this tarek leitner just for a misguided whore son? #orf #confrontations', 'target_text': 'What is this tarek leiter just for a person? #orf #confrontations'}, {'input_text': \"detoxify: It's disgusting, this Schmidt. Without regard for the bees. This. piece of conscience must be voted off and chased away.\", 'target_text': \"I'm sorry, this Schmidt, without regard for the bees, this person has to be voted off.\"}, {'input_text': 'detoxify: If Turks occupy the airports and party headquarters, prove the indignation! Your hypocrite', 'target_text': 'If Turks occupy the airports and party headquarters, prove the indignation great!'}, {'input_text': \"detoxify: IT WOULDN'T BE NEEDED TO REMEMBER THESE WHEN THE ELITARIANS ARE NOT EXCLUDE!!! Those who only enslave us !!!\", 'target_text': \"IT WOULDN'T BE NEEDED TO REMEMBER THESE WILLN'T BE THE ELITARIAN DEPARTMENT !!! Those who are only oppressing us !!!\"}, {'input_text': \"detoxify: All in all, there's only stupid people sitting there, starting with the presenter, who's even dumber.\", 'target_text': \"All in all, there's only funny people sitting there, starting with the presenter, who's the worst.\"}, {'input_text': \"detoxify: Merkel's vassals speak up!. Mr. Gersemann is a vassal of Merkel and his comments are given by Merkel!!!\", 'target_text': \"Merkel's supporters speak up!. Mr. Gersemann is a supporter of Merkel and his comments are given by Merkel!!!\"}, {'input_text': \"detoxify: And since I'm not a little girl, as you seem, I can give up such an opinion.\", 'target_text': \"And since I'm not as sensitive as you seem, I can give up such an opinion.\"}, {'input_text': 'detoxify: All you old trash cans still live...thought the time has blessed you.... Are just a tough weed root', 'target_text': \"All you're still alive...thought the temporal has blessed you... Are just a tough personality\"}, {'input_text': 'detoxify: It is enough for me to know this coward from Rostock in 1989/90. He was pushed into the \"light\", this Gauckler', 'target_text': 'It is enough for me to know this man from Rostock in 1989/90. He was pushed into the \"light\", this Gauck'}, {'input_text': 'detoxify: I want him for a short time... but this Bayer is really a washcloth.', 'target_text': 'I want him for a short time... but this Bayer is really more passive'}, {'input_text': 'detoxify: In kindergartens next to the children? would be even more beautiful. Trembling this asylum seeker pack in tents away from civilization', 'target_text': 'In kindergartens next to the children? would be even more beautiful.'}, {'input_text': 'detoxify: As long as politicians of the thin shot come out of the mouth, nothing will change. It stinks further !!!', 'target_text': 'As long as the politicians talk nonsense, nothing will change.'}, {'input_text': \"detoxify: I just can't stand this shit with these drift migrants anymore......if I'm just gonna see that cracking creepy......fucking shit..!\", 'target_text': \"I just can't stand it with these migrants anymore... if I see any of them already...\"}, {'input_text': \"detoxify: Merkel's stupid Germans can be told everything!!\", 'target_text': \"Merkel's Germans can be told everything!!\"}, {'input_text': 'detoxify: Nazi lie press and Merkel vassals continue to rush against Trump!!', 'target_text': 'Press and Merkel supporters continue to rush against Trump!!'}, {'input_text': 'detoxify: Selling arrests of rapists as a success is the annoyance of a poor authority. #Police #Cologne #Rapefugees', 'target_text': 'Selling arrests of rapists as a success is the annoyance of a poor authority. #Police #Cologne'}, {'input_text': 'detoxify: Why does an AFD bird have to be there?', 'target_text': 'Why does one of the AFD have to be there?'}, {'input_text': \"detoxify: Fucked chicken shit. But it's clear that someone from Germany's hypocritical party thinks that way. SPD - SCHMAROTZER, PEDOPHILE and DENUNZIANTS.. Who betrayed us....\", 'target_text': \"But it's clear that someone in this party thinks that way, SPD... who betrayed us...\"}]\n",
      "<class 'list'>\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import json\n",
    "\n",
    "# Load raw list of strings\n",
    "with open(\"translated_de_to_en_pairs.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_texts = json.load(f)\n",
    "\n",
    "print(\"Raw texts loaded:\", (raw_texts))\n",
    "\n",
    "# Wrap each string in a dictionary\n",
    "data = [\n",
    "    {\n",
    "        \"input_text\": entry.get(\"input_text\", \"\"),\n",
    "        \"target_text\": entry.get(\"target_text\", \"\")\n",
    "    }\n",
    "    for entry in raw_texts\n",
    "]\n",
    "\n",
    "eval_dataset_de = data\n",
    "\n",
    "print(type(eval_dataset_de))         \n",
    "print(type(eval_dataset_de[0]))      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e75a23",
   "metadata": {},
   "source": [
    "# Detoxification of Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f56a3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import T5Tokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_and_save_detoxification(\n",
    "    model,\n",
    "    dataset,\n",
    "    output_path,\n",
    "    tokenizer=None,\n",
    "    batch_size=12,\n",
    "    lang=\"en\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluates a detoxification model and saves results to a JSON file.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained T5 detoxification model.\n",
    "        dataset: HuggingFace Dataset with \"input_text\" and \"target_text\".\n",
    "        output_path: Path to save the JSON results.\n",
    "        tokenizer: Optional tokenizer, default is T5.\n",
    "        batch_size: Batch size for generation.\n",
    "        lang: Language tag for progress bar.\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    tokenizer = tokenizer or T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "    def collate_fn(batch):\n",
    "        texts = [ex[\"input_text\"] for ex in batch]\n",
    "        return tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
    "\n",
    "    input_texts = []\n",
    "    reference_texts = []\n",
    "    detoxified_outputs = []\n",
    "\n",
    "    model.eval()\n",
    "    for i, batch in enumerate(tqdm(loader, desc=f\"Generating Detoxified ({lang})\")):\n",
    "        batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "        outputs = model.generate(\n",
    "            **batch,\n",
    "            max_length=50,\n",
    "            num_beams=4,\n",
    "            early_stopping=True,\n",
    "            decoder_start_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "\n",
    "        decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        detoxified_outputs.extend(decoded)\n",
    "\n",
    "        for j in range(len(decoded)):\n",
    "            example = dataset[i * batch_size + j]\n",
    "            input_texts.append(example[\"input_text\"])\n",
    "            reference_texts.append(example.get(\"target_text\", \"\"))\n",
    "\n",
    "    # Save results to JSON\n",
    "    result = {\n",
    "        \"input_texts\": input_texts,\n",
    "        \"reference_texts\": reference_texts,\n",
    "        \"detoxified_outputs\": detoxified_outputs\n",
    "    }\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(result, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"✅ Saved detoxified outputs to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da0f084d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Detoxified (en): 100%|██████████| 34/34 [00:13<00:00,  2.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved detoxified outputs to detoxified_outputs_en.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# For English\n",
    "evaluate_and_save_detoxification(\n",
    "    model=model,\n",
    "    dataset=eval_dataset,\n",
    "    output_path=\"detoxified_outputs_en.json\",\n",
    "    tokenizer=tokenizer,\n",
    "    lang=\"en\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8526b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Detoxified (de): 100%|██████████| 34/34 [00:21<00:00,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved detoxified outputs to detoxified_outputs_de.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# For German\n",
    "evaluate_and_save_detoxification(\n",
    "    model=model,\n",
    "    dataset=eval_dataset_de,  # or eval_dataset\n",
    "    output_path=\"detoxified_outputs_de.json\",\n",
    "    tokenizer=tokenizer,\n",
    "    lang=\"de\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da255627",
   "metadata": {},
   "source": [
    "## Translation English to German"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "99aeb520",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonaz\\git\\Maria_stuff\\Maria_code\\.venv\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "Translating: 100%|██████████| 50/50 [00:25<00:00,  2.00it/s]\n",
      "Translating: 100%|██████████| 50/50 [00:24<00:00,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Translated dataset saved as 'evaluation_dataset_german.json'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# Load the English-to-German translation model and tokenizer\n",
    "model_name = \"Helsinki-NLP/opus-mt-en-de\"\n",
    "tokenizer_mt = MarianTokenizer.from_pretrained(model_name)\n",
    "model_mt = MarianMTModel.from_pretrained(model_name).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the English dataset\n",
    "with open(\"translated_de_to_en_pairs.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data_en = json.load(f)\n",
    "\n",
    "# Extract and clean the input and target texts\n",
    "toxic_en = [entry[\"input_text\"].replace(\"detoxify: \", \"\") for entry in data_en]\n",
    "neutral_en = [entry[\"target_text\"] for entry in data_en]\n",
    "\n",
    "# Helper function to batch translate a list of strings\n",
    "def batch_translate(texts, batch_size=8):\n",
    "    translations = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Translating\"):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        inputs = tokenizer_mt(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=128).to(model_mt.device)\n",
    "        outputs = model_mt.generate(**inputs, max_length=128)\n",
    "        decoded = tokenizer_mt.batch_decode(outputs, skip_special_tokens=True)\n",
    "        translations.extend(decoded)\n",
    "    return translations\n",
    "\n",
    "# Translate both toxic and neutral texts\n",
    "toxic_de = batch_translate(toxic_en)\n",
    "neutral_de = batch_translate(neutral_en)\n",
    "\n",
    "# Reconstruct the translated dataset without the \"detoxify: \" prefix\n",
    "translated_data = [\n",
    "    {\"input_text\": toxic, \"target_text\": neutral}\n",
    "    for toxic, neutral in zip(toxic_de, neutral_de)\n",
    "]\n",
    "\n",
    "# Save the translated dataset to a JSON file\n",
    "with open(\"evaluation_dataset_german.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(translated_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"✅ Translated dataset saved as 'evaluation_dataset_german.json'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4a4682",
   "metadata": {},
   "source": [
    "# Test Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c1e362",
   "metadata": {},
   "source": [
    "## Style Transfer Accuracy - Toxicity Level Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d1687b",
   "metadata": {},
   "source": [
    "### reformatting detoxified outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3a3e339b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved as 'evaluation_dataset_english.json'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the detoxified results\n",
    "with open(\"detoxified_results.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract and clean input and output texts\n",
    "input_texts = [text.replace(\"detoxify: \", \"\").strip() for text in data[\"input_texts\"]]\n",
    "detoxified_outputs = data[\"detoxified_outputs\"]\n",
    "\n",
    "# Create the reformatted structure\n",
    "reformatted = [\n",
    "    {\"input_text\": input_text, \"target_text\": detoxified}\n",
    "    for input_text, detoxified in zip(input_texts, detoxified_outputs)\n",
    "]\n",
    "\n",
    "# Save to file\n",
    "with open(\"evaluation_dataset_english.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(reformatted, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"✅ Saved as 'evaluation_dataset_english.json'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c1ae3ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_toxicity_from_list_of_dicts(file_path: str, text_key: str = \"target_text\"):\n",
    "    \"\"\"\n",
    "    Evaluates toxicity for a JSON list of dictionaries (e.g., translated detoxified texts).\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the JSON file.\n",
    "        text_key (str): Key used to extract text from each dictionary.\n",
    "    \"\"\"\n",
    "    # Load JSON file\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        dataset = json.load(f)\n",
    "\n",
    "    # Extract texts using specified key\n",
    "    texts = [entry[text_key] for entry in dataset]\n",
    "    print(f\"✅ Loaded {len(texts)} texts from {file_path}\")\n",
    "\n",
    "    # Run classification in batches\n",
    "    batch_size = 16\n",
    "    toxic_count = 0\n",
    "    non_toxic_count = 0\n",
    "\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        predictions = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "        for label in predictions:\n",
    "            if label.item() == 0:\n",
    "                non_toxic_count += 1\n",
    "            else:\n",
    "                toxic_count += 1\n",
    "\n",
    "    # Print summary\n",
    "    total = toxic_count + non_toxic_count\n",
    "    print(\"\\n--- Toxicity Evaluation Summary ---\")\n",
    "    print(f\"Total texts evaluated: {total}\")\n",
    "    print(f\"Non-toxic: {non_toxic_count} ({(non_toxic_count / total) * 100:.2f}%)\")\n",
    "    print(f\"Toxic: {toxic_count} ({(toxic_count / total) * 100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fa934cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 400 texts from evaluation_dataset_german.json\n",
      "\n",
      "--- Toxicity Evaluation Summary ---\n",
      "Total texts evaluated: 400\n",
      "Non-toxic: 320 (80.00%)\n",
      "Toxic: 80 (20.00%)\n",
      "✅ Loaded 400 texts from evaluation_dataset_english.json\n",
      "\n",
      "--- Toxicity Evaluation Summary ---\n",
      "Total texts evaluated: 400\n",
      "Non-toxic: 276 (69.00%)\n",
      "Toxic: 124 (31.00%)\n"
     ]
    }
   ],
   "source": [
    "evaluate_toxicity_from_list_of_dicts(\"evaluation_dataset_german.json\", text_key=\"target_text\")\n",
    "evaluate_toxicity_from_list_of_dicts(\"evaluation_dataset_english.json\", text_key=\"target_text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ed0b45",
   "metadata": {},
   "source": [
    "## Content preservation: LaBSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ccf79500",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load LaBSE model once\n",
    "labse_model = SentenceTransformer(\"sentence-transformers/LaBSE\")\n",
    "\n",
    "def compute_labse_similarity_from_file(filepath, input_key=\"input_text\", target_key=\"target_text\", sample_size=None, batch_size=16):\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    if sample_size:\n",
    "        data = data[:sample_size]\n",
    "\n",
    "    input_texts = [entry[input_key] for entry in data]\n",
    "    target_texts = [entry[target_key] for entry in data]\n",
    "\n",
    "    similarities = []\n",
    "\n",
    "    for i in tqdm(range(0, len(input_texts), batch_size), desc=\"Computing LaBSE Similarities\"):\n",
    "        batch_input = input_texts[i:i+batch_size]\n",
    "        batch_target = target_texts[i:i+batch_size]\n",
    "\n",
    "        emb_input = labse_model.encode(batch_input, convert_to_tensor=True)\n",
    "        emb_target = labse_model.encode(batch_target, convert_to_tensor=True)\n",
    "\n",
    "        sim = torch.nn.functional.cosine_similarity(emb_input, emb_target).cpu().numpy()\n",
    "        similarities.extend(sim)\n",
    "\n",
    "    avg_sim = float(np.mean(similarities))\n",
    "    print(f\"✅ {filepath} — Content Preservation (LaBSE): {avg_sim:.4f}\")\n",
    "    return avg_sim, similarities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ca56e4c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing LaBSE Similarities: 100%|██████████| 7/7 [00:00<00:00, 17.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ evaluation_dataset_english.json — Content Preservation (LaBSE): 0.9051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing LaBSE Similarities: 100%|██████████| 7/7 [00:00<00:00, 30.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ evaluation_dataset_german.json — Content Preservation (LaBSE): 0.9001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9000511765480042,\n",
       " [np.float32(0.83013844),\n",
       "  np.float32(0.8651395),\n",
       "  np.float32(0.9469372),\n",
       "  np.float32(0.8971749),\n",
       "  np.float32(0.89105636),\n",
       "  np.float32(0.9737712),\n",
       "  np.float32(0.9504341),\n",
       "  np.float32(0.7288624),\n",
       "  np.float32(0.7319522),\n",
       "  np.float32(0.780321),\n",
       "  np.float32(0.9135866),\n",
       "  np.float32(0.88522166),\n",
       "  np.float32(0.98429847),\n",
       "  np.float32(0.9063605),\n",
       "  np.float32(0.9041235),\n",
       "  np.float32(0.9013325),\n",
       "  np.float32(0.8859218),\n",
       "  np.float32(0.5979222),\n",
       "  np.float32(0.91307706),\n",
       "  np.float32(0.9871712),\n",
       "  np.float32(0.86609435),\n",
       "  np.float32(0.91994584),\n",
       "  np.float32(0.9342329),\n",
       "  np.float32(0.8082813),\n",
       "  np.float32(0.8509568),\n",
       "  np.float32(0.8449616),\n",
       "  np.float32(0.97551537),\n",
       "  np.float32(0.83295226),\n",
       "  np.float32(0.9817647),\n",
       "  np.float32(0.97704446),\n",
       "  np.float32(0.7917104),\n",
       "  np.float32(0.7877049),\n",
       "  np.float32(0.9449831),\n",
       "  np.float32(0.93468344),\n",
       "  np.float32(0.9247023),\n",
       "  np.float32(0.9761138),\n",
       "  np.float32(0.85103923),\n",
       "  np.float32(0.91977817),\n",
       "  np.float32(1.0),\n",
       "  np.float32(0.9630654),\n",
       "  np.float32(0.7836719),\n",
       "  np.float32(0.94816136),\n",
       "  np.float32(0.9553695),\n",
       "  np.float32(0.8491082),\n",
       "  np.float32(0.8441507),\n",
       "  np.float32(0.9642109),\n",
       "  np.float32(0.9019956),\n",
       "  np.float32(0.9204768),\n",
       "  np.float32(0.96231204),\n",
       "  np.float32(0.9952338),\n",
       "  np.float32(1.0000001),\n",
       "  np.float32(0.8444406),\n",
       "  np.float32(0.8408959),\n",
       "  np.float32(0.8714229),\n",
       "  np.float32(0.8911909),\n",
       "  np.float32(0.9318404),\n",
       "  np.float32(0.7964711),\n",
       "  np.float32(0.949321),\n",
       "  np.float32(0.976206),\n",
       "  np.float32(0.9245879),\n",
       "  np.float32(0.9131739),\n",
       "  np.float32(0.9794826),\n",
       "  np.float32(0.91505665),\n",
       "  np.float32(0.9036759),\n",
       "  np.float32(0.74146193),\n",
       "  np.float32(0.7617422),\n",
       "  np.float32(0.8571295),\n",
       "  np.float32(0.9835073),\n",
       "  np.float32(1.0),\n",
       "  np.float32(1.0),\n",
       "  np.float32(0.9045411),\n",
       "  np.float32(0.8976059),\n",
       "  np.float32(0.94233847),\n",
       "  np.float32(0.9774257),\n",
       "  np.float32(0.9335453),\n",
       "  np.float32(0.95390654),\n",
       "  np.float32(0.9038873),\n",
       "  np.float32(0.8823525),\n",
       "  np.float32(0.77437705),\n",
       "  np.float32(0.8726368),\n",
       "  np.float32(0.9518427),\n",
       "  np.float32(0.776608),\n",
       "  np.float32(0.9847972),\n",
       "  np.float32(0.95951253),\n",
       "  np.float32(0.9869886),\n",
       "  np.float32(0.85219157),\n",
       "  np.float32(0.9393606),\n",
       "  np.float32(0.8975791),\n",
       "  np.float32(0.950771),\n",
       "  np.float32(0.9579543),\n",
       "  np.float32(0.9782977),\n",
       "  np.float32(0.90427613),\n",
       "  np.float32(0.7076921),\n",
       "  np.float32(0.92502797),\n",
       "  np.float32(0.89416134),\n",
       "  np.float32(0.9374003),\n",
       "  np.float32(0.8192028),\n",
       "  np.float32(0.8421292),\n",
       "  np.float32(0.9564479),\n",
       "  np.float32(0.9736142)])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_labse_similarity_from_file(\"evaluation_dataset_english.json\", sample_size=100)\n",
    "compute_labse_similarity_from_file(\"evaluation_dataset_german.json\", sample_size=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1741ea0f",
   "metadata": {},
   "source": [
    "## BLEU Evaluation Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e923430f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU Score: 0.5159\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "#nltk.download(\"punkt_tab\")\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "smooth_fn = SmoothingFunction().method4\n",
    "\n",
    "bleu_scores = []\n",
    "\n",
    "for hyp, ref in zip(detoxified_outputs, reference_texts):\n",
    "    hyp_tokens = nltk.word_tokenize(hyp)\n",
    "    ref_tokens = nltk.word_tokenize(ref)\n",
    "    \n",
    "    score = sentence_bleu(\n",
    "        [ref_tokens],\n",
    "        hyp_tokens,\n",
    "        smoothing_function=smooth_fn\n",
    "    )\n",
    "    bleu_scores.append(score)\n",
    "\n",
    "avg_bleu = np.mean(bleu_scores)\n",
    "print(f\"Average BLEU Score: {avg_bleu:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb2a4f4",
   "metadata": {},
   "source": [
    "## Bert-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3d867045",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "180002e1a26f4cac82ab6cef68945024",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a88f98ec1ad74337a1e99f893d2f152b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.95 seconds, 204.89 sentences/sec\n",
      "Average BERTScore Precision:  0.9443\n",
      "Average BERTScore Recall:     0.9455\n",
      "Average BERTScore F1:         0.9447\n"
     ]
    }
   ],
   "source": [
    "from bert_score import score\n",
    "\n",
    "# Use your lists from earlier\n",
    "# detoxified_outputs = [...]   # model-generated sentences\n",
    "# reference_texts = [...]      # ground truth neutral sentences\n",
    "\n",
    "P, R, F1 = score(detoxified_outputs, reference_texts, lang=\"en\", verbose=True)\n",
    "\n",
    "# Average scores\n",
    "print(f\"Average BERTScore Precision:  {P.mean():.4f}\")\n",
    "print(f\"Average BERTScore Recall:     {R.mean():.4f}\")\n",
    "print(f\"Average BERTScore F1:         {F1.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5a9f7d",
   "metadata": {},
   "source": [
    "## Toxicity Score Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8db40395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:    detoxify: #Land of the idiots #mental illness - \"fight gg #Trump\" (gg #Putin anyway) German # Regime in megalomania ....\n",
      "Detoxified:  #Land of the idiots #mental illness - \"fight gg #Trump\" (gg #Putin anyway) German # Regime in megalomania\n",
      "Toxicity Before: 0.98\n",
      "Toxicity After:  0.99\n",
      "--------------------------------------------------\n",
      "Original:    detoxify: #Lindner's got his head in the ass of #Merkel disgusting #FDP tonight\n",
      "Detoxified:  #Lindner's got his head in the way of #Merkel disgusting #FDP tonight\n",
      "Toxicity Before: 0.98\n",
      "Toxicity After:  0.42\n",
      "--------------------------------------------------\n",
      "Original:    detoxify: Whether it's Merkel or Schulz? We Germans don't want to and can't hear this moron of plague (Merkel) and cholera (Schulz) anymore!\n",
      "Detoxified:  Whether it's Merkel or Schulz? We Germans don't want to and cannot hear this moron of plague (Merkel) and cholera (Schulz) anymore!\n",
      "Toxicity Before: 0.98\n",
      "Toxicity After:  0.99\n",
      "--------------------------------------------------\n",
      "Average Toxicity Before: 0.45\n",
      "Average Toxicity After: 0.24\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from detoxify import Detoxify\n",
    "\n",
    "# Load Detoxify model\n",
    "toxicity_model = Detoxify('unbiased')\n",
    "\n",
    "# Run toxicity prediction\n",
    "toxicity_scores = toxicity_model.predict(input_texts)\n",
    "detoxified_scores = toxicity_model.predict(detoxified_outputs)\n",
    "\n",
    "# Print sample comparisons\n",
    "for i in range(3):\n",
    "    print(f\"Original:    {input_texts[i]}\")\n",
    "    print(f\"Detoxified:  {detoxified_outputs[i]}\")\n",
    "    print(f\"Toxicity Before: {toxicity_scores['toxicity'][i]:.2f}\")\n",
    "    print(f\"Toxicity After:  {detoxified_scores['toxicity'][i]:.2f}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate average toxicity\n",
    "avg_toxicity_before = np.mean(toxicity_scores['toxicity'])\n",
    "avg_toxicity_after = np.mean(detoxified_scores['toxicity'])\n",
    "\n",
    "print(f\"Average Toxicity Before: {avg_toxicity_before:.2f}\")\n",
    "print(f\"Average Toxicity After: {avg_toxicity_after:.2f}\")\n",
    "\n",
    "# Plotting the distributions\n",
    "plt.hist(toxicity_scores['toxicity'], bins=20, alpha=0.5, label='Before Detox')\n",
    "plt.hist(detoxified_scores['toxicity'], bins=20, alpha=0.5, label='After Detox')\n",
    "plt.xlabel('Toxicity Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Toxicity Score Distribution (Refined Model)')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6792c885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t = 30.232, p = 0.00000\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import ttest_rel\n",
    "\n",
    "# Example: compare two arrays of toxicity scores\n",
    "before = np.array(toxicity_scores['toxicity'])\n",
    "after = np.array(detoxified_scores['toxicity'])\n",
    "\n",
    "t_stat, p_value = ttest_rel(before, after)\n",
    "print(f\"t = {t_stat:.3f}, p = {p_value:.5f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
