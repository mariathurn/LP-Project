{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f1df8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0. Install dependencies if not already installed\n",
    "# !pip install datasets transformers detoxify torch pandas tqdm\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from transformers import (\n",
    "    T5Tokenizer, T5ForConditionalGeneration,\n",
    "    Seq2SeqTrainingArguments, Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq, EarlyStoppingCallback\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "from detoxify import Detoxify\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1. Load datasets\n",
    "print(\"Loading datasets...\")\n",
    "paradetox = load_dataset(\"textdetox/multilingual_paradetox\", split=\"en\")\n",
    "snlp = load_dataset(\"s-nlp/paradetox\", split=\"train\")\n",
    "\n",
    "# 2. Preprocess to extract English input-output pairs\n",
    "def preprocess_paradetox(example):\n",
    "    return {\n",
    "        \"input_text\": \"detoxify: \" + example[\"toxic_sentence\"],\n",
    "        \"target_text\": example[\"neutral_sentence\"]\n",
    "    }\n",
    "\n",
    "def preprocess_snlp(example):\n",
    "    return {\n",
    "        \"input_text\": \"detoxify: \" + example[\"en_toxic_comment\"],\n",
    "        \"target_text\": example[\"en_neutral_comment\"]\n",
    "    }\n",
    "\n",
    "paradetox = paradetox.map(preprocess_paradetox)\n",
    "snlp = snlp.map(preprocess_snlp)\n",
    "\n",
    "# 3. Keep only required columns\n",
    "def clean_columns(ds):\n",
    "    return ds.remove_columns([col for col in ds.column_names if col not in [\"input_text\", \"target_text\"]])\n",
    "\n",
    "paradetox = clean_columns(paradetox)\n",
    "snlp = clean_columns(snlp)\n",
    "\n",
    "# 4. Combine and filter\n",
    "combined = concatenate_datasets([paradetox, snlp])\n",
    "combined = combined.filter(lambda x: x[\"input_text\"] is not None and x[\"target_text\"] is not None)\n",
    "\n",
    "# 5. Tokenize\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "def tokenize(example):\n",
    "    inputs = tokenizer(example[\"input_text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(example[\"target_text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    labels[\"input_ids\"] = [\n",
    "        [(tok if tok != tokenizer.pad_token_id else -100) for tok in seq]\n",
    "        for seq in labels[\"input_ids\"]\n",
    "    ]\n",
    "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "tokenized = combined.map(tokenize, batched=True)\n",
    "\n",
    "# 6. Split into train/test\n",
    "split = tokenized.train_test_split(test_size=0.1)\n",
    "train_dataset = split[\"train\"]\n",
    "eval_dataset = split[\"test\"]\n",
    "\n",
    "# Keep original test for evaluation\n",
    "original_eval = combined.train_test_split(test_size=0.1)[\"test\"]\n",
    "\n",
    "# 7. Load model\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "model.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bf56adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import Seq2SeqTrainer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ToxicityPenaltyTrainer(Seq2SeqTrainer):\n",
    "    def __init__(self, *args, lambda_penalty=0.5, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.lambda_penalty = lambda_penalty\n",
    "\n",
    "        # Load fast toxicity classifier\n",
    "        self.tox_tokenizer = AutoTokenizer.from_pretrained(\"unitary/toxic-bert\")\n",
    "        self.tox_model = AutoModelForSequenceClassification.from_pretrained(\"unitary/toxic-bert\").to(self.model.device)\n",
    "        self.tox_model.eval()\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # Standard generation loss\n",
    "        outputs = model(**inputs)\n",
    "        generation_loss = outputs.loss\n",
    "\n",
    "        # Decode model predictions\n",
    "        generated_ids = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=50,\n",
    "            num_beams=1,\n",
    "            early_stopping=True,\n",
    "            decoder_start_token_id=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        decoded_texts = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "        # Tokenize generated text for classifier\n",
    "        tox_inputs = self.tox_tokenizer(decoded_texts, return_tensors=\"pt\", truncation=True, padding=True).to(self.model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            tox_logits = self.tox_model(**tox_inputs).logits\n",
    "            tox_probs = torch.sigmoid(tox_logits[:, 0])  # binary: higher means more toxic\n",
    "\n",
    "        # Average toxicity as penalty\n",
    "        penalty = tox_probs.mean()\n",
    "\n",
    "        # Combine losses\n",
    "        total_loss = generation_loss + self.lambda_penalty * penalty\n",
    "        return (total_loss, outputs) if return_outputs else total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "096c934c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "--load_best_model_at_end requires the save and eval strategy to match, but found\n- Evaluation strategy: IntervalStrategy.NO\n- Save strategy: SaveStrategy.EPOCH",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 8. Training config\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m args = \u001b[43mSeq2SeqTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./mt5-detox\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mepoch\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpredict_with_generate\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_best_model_at_end\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetric_for_best_model\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meval_loss\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgreater_is_better\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_total_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./logs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreport_to\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnone\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfp16\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m trainer = ToxicityPenaltyTrainer(\n\u001b[32m     21\u001b[39m     model=model,\n\u001b[32m     22\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m     28\u001b[39m     lambda_penalty=\u001b[32m0.5\u001b[39m  \u001b[38;5;66;03m# Tune this value!\u001b[39;00m\n\u001b[32m     29\u001b[39m )\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# 9. Train\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:137\u001b[39m, in \u001b[36m__init__\u001b[39m\u001b[34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, torch_empty_cache_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, tp_size, fsdp_transformer_layer_cls_to_wrap, accelerator_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, include_for_metrics, eval_do_concat_batches, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, batch_eval_metrics, eval_on_start, use_liger_kernel, eval_use_gather_object, average_tokens_across_devices, sortish_sampler, predict_with_generate, generation_max_length, generation_num_beams, generation_config)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jonaz\\Maria_code\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1648\u001b[39m, in \u001b[36mTrainingArguments.__post_init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1646\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.load_best_model_at_end \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.save_strategy != SaveStrategy.BEST:\n\u001b[32m   1647\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.eval_strategy != \u001b[38;5;28mself\u001b[39m.save_strategy:\n\u001b[32m-> \u001b[39m\u001b[32m1648\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1649\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m--load_best_model_at_end requires the save and eval strategy to match, but found\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m- Evaluation \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1650\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mstrategy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.eval_strategy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m- Save strategy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.save_strategy\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1651\u001b[39m         )\n\u001b[32m   1652\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.eval_strategy == IntervalStrategy.STEPS \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.save_steps % \u001b[38;5;28mself\u001b[39m.eval_steps != \u001b[32m0\u001b[39m:\n\u001b[32m   1653\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.eval_steps < \u001b[32m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.save_steps < \u001b[32m1\u001b[39m:\n",
      "\u001b[31mValueError\u001b[39m: --load_best_model_at_end requires the save and eval strategy to match, but found\n- Evaluation strategy: IntervalStrategy.NO\n- Save strategy: SaveStrategy.EPOCH"
     ]
    }
   ],
   "source": [
    "# 8. Training config\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./mt5-detox\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=5,\n",
    "    predict_with_generate=True,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    report_to=\"none\",\n",
    "    fp16=torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "trainer = ToxicityPenaltyTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    "    lambda_penalty=0.5  # Tune this value!\n",
    ")\n",
    "\n",
    "# 9. Train\n",
    "print(\"Training model...\")\n",
    "trainer.train()\n",
    "\n",
    "# 10. Generate detoxified outputs\n",
    "def collate_fn(batch):\n",
    "    texts = [ex[\"input_text\"] for ex in batch]\n",
    "    return tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "\n",
    "loader = DataLoader(original_eval, batch_size=16, collate_fn=collate_fn)\n",
    "model.eval()\n",
    "\n",
    "detoxified_outputs = []\n",
    "input_texts = []\n",
    "reference_texts = []\n",
    "\n",
    "print(\"Generating detoxified outputs...\")\n",
    "for i, batch in enumerate(tqdm(loader)):\n",
    "    batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "    outputs = model.generate(**batch, max_length=50, num_beams=4, early_stopping=True, decoder_start_token_id=tokenizer.pad_token_id)\n",
    "    decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    detoxified_outputs.extend(decoded)\n",
    "    for j in range(len(decoded)):\n",
    "        idx = i * 16 + j\n",
    "        if idx < len(original_eval):\n",
    "            input_texts.append(original_eval[idx][\"input_text\"])\n",
    "            reference_texts.append(original_eval[idx][\"target_text\"])\n",
    "\n",
    "# 11. Evaluate with Detoxify\n",
    "print(\"Scoring toxicity before and after...\")\n",
    "tox_model = Detoxify(\"unbiased\")\n",
    "tox_before = tox_model.predict(input_texts)[\"toxicity\"]\n",
    "tox_after = tox_model.predict(detoxified_outputs)[\"toxicity\"]\n",
    "\n",
    "# 12. Save and summarize\n",
    "df = pd.DataFrame({\n",
    "    \"toxic_input\": input_texts,\n",
    "    \"reference_output\": reference_texts,\n",
    "    \"model_output\": detoxified_outputs,\n",
    "    \"toxicity_before\": tox_before,\n",
    "    \"toxicity_after\": tox_after\n",
    "})\n",
    "\n",
    "df.to_csv(\"detoxified_results.csv\", index=False)\n",
    "print(\"\\n✅ Results saved to 'detoxified_results.csv'\")\n",
    "\n",
    "# Summary\n",
    "print(f\"\\nAverage Toxicity Before: {sum(tox_before)/len(tox_before):.4f}\")\n",
    "print(f\"Average Toxicity After:  {sum(tox_after)/len(tox_after):.4f}\")\n",
    "print(f\"Average Reduction:       {sum(t - d for t, d in zip(tox_before, tox_after))/len(tox_after):.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
