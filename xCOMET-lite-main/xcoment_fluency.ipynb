{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38878e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.backends.mps.is_available()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10ffbe65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from xcomet.deberta_encoder import XCOMETLite\n",
    "from typing import Optional\n",
    "\n",
    "def evaluate_fluency_with_xcomet(\n",
    "    input_path: str,\n",
    "    output_path: Optional[str] = \"xcomet_fluency_scores.json\",\n",
    "    model_name: str = \"myyycroft/XCOMET-lite\",\n",
    "    batch_size: int = 2,\n",
    "    num_workers: int = 0,\n",
    "    accelerator: str = \"auto\",\n",
    "    devices: list = [0]\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Loads input data, runs XCOMET fluency prediction, and optionally saves scores.\n",
    "\n",
    "    Args:\n",
    "        input_path (str): Path to the JSON input file.\n",
    "        output_path (str, optional): Where to save the detailed scores. Set to None to skip saving.\n",
    "        model_name (str): Hugging Face model name or local path.\n",
    "        batch_size (int): Batch size for prediction.\n",
    "        num_workers (int): Number of worker processes (0 for none).\n",
    "        accelerator (str): Accelerator type (e.g., \"auto\", \"cpu\", \"gpu\").\n",
    "        devices (list): List of device IDs.\n",
    "\n",
    "    Returns:\n",
    "        float: The average fluency score.\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Load model\n",
    "    model = XCOMETLite.from_pretrained(model_name)\n",
    "\n",
    "    # Predict\n",
    "    result = model.predict(\n",
    "        data,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        accelerator=accelerator,\n",
    "        devices=devices\n",
    "    )\n",
    "\n",
    "    # Save scores\n",
    "    if output_path:\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as out:\n",
    "            json.dump(result.scores, out, indent=2)\n",
    "\n",
    "    # Print and return average\n",
    "    avg_score = sum(result.scores) / len(result.scores)\n",
    "    print(f\"Average Fluency Score: {avg_score}\")\n",
    "    return avg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d55cd3d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/metric/lib/python3.11/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/metric/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Encoder model frozen.\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Predicting DataLoader 0: 100%|██████████| 200/200 [01:12<00:00,  2.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Fluency Score: 0.8999665227532386\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8999665227532386"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_fluency_with_xcomet(\"xcomet_input_data_de.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5cee40f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/metric/lib/python3.11/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/metric/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Encoder model frozen.\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Predicting DataLoader 0: 100%|██████████| 200/200 [00:43<00:00,  4.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Fluency Score: 0.8715424510091543\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8715424510091543"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_fluency_with_xcomet(\"xcomet_input_data_en.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d40b437",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/metric/lib/python3.11/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/metric/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Encoder model frozen.\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Predicting DataLoader 0: 100%|██████████| 200/200 [01:06<00:00,  3.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Fluency Score: 0.9424139831960201\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9424139831960201"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_fluency_with_xcomet(\"xcomet_input_data_de_base.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3952deca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/metric/lib/python3.11/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/metric/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Encoder model frozen.\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Predicting DataLoader 0: 100%|██████████| 200/200 [00:43<00:00,  4.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Fluency Score: 0.6290126404166222\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6290126404166222"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_fluency_with_xcomet(\"xcomet_input_data_en_base.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597ecae3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
